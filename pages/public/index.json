[{"uri":"/","title":"AWS Datalab-ç»†ç²’åº¦æƒ…æ„Ÿåˆ†æ","tags":[],"description":"","content":"Author\n JUNYI LIU (AWS GCR Applied Scientist)  æ¦‚è¿° æœ¬æ¬¡workshopåˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†\n èƒŒæ™¯ä»‹ç»- what is text summaryï¼Ÿ åŸºäºAmazon SageMakerçš„TEXTRANKæ¨¡å‹è®­ç»ƒåŠ¨æ‰‹å®éªŒ åŸºäºAmazon SageMakerçš„Huggingfaceè®­ç»ƒä¸€ä¸ªBARTè‹±æ–‡æ‘˜è¦æ¨¡å‹  æ¨¡å‹è®­ç»ƒÃŸ æ¨¡å‹éƒ¨ç½²   åŸºäºAmazon SageMakerçš„MT5æ¨¡å‹è®­ç»ƒåŠ¨æ‰‹å®éªŒ  æœ¬åœ°æ¨¡å‹è®­ç»ƒ å¯åŠ¨æ¨¡å‹è®­ç»ƒä»»åŠ¡ æ¨¡å‹éƒ¨ç½²   åŸºäºAmazon SageMakerçš„CPTæ¨¡å‹è®­ç»ƒåŠ¨æ‰‹å®éªŒ  æ¨¡å‹è®­ç»ƒ æ¨¡å‹å¢å¼ºè®­ç»ƒ æ¨¡å‹éƒ¨ç½²    æœ¬æ¬¡ workshop å‰æ æœ¬æ¬¡ workshop å»ºè®®åœ¨ US-WEST-2 Region ä½¿ç”¨ã€‚ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæ‰€ä»¥æœ¬ workshop æ‰€æœ‰çš„æ¼”ç¤ºéƒ½ä¼šä»¥US-WEST-2 Region ä¸ºä¾‹ã€‚\n"},{"uri":"/01introduction.html","title":"Introduction","tags":[],"description":"","content":"text summaryï¼ˆæ–‡æ¡£æ‘˜è¦ï¼‰ éšç€äº’è”ç½‘äº§ç”Ÿçš„æ–‡æœ¬æ•°æ®è¶Šæ¥è¶Šå¤šï¼Œæ–‡æœ¬ä¿¡æ¯è¿‡è½½é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œå¯¹å„ç±»æ–‡æœ¬è¿›è¡Œä¸€ä¸ªâ€œé™ ç»´â€å¤„ç†æ˜¾å¾—éå¸¸å¿…è¦ï¼Œæ–‡æœ¬æ‘˜è¦ä¾¿æ˜¯å…¶ä¸­ä¸€ä¸ªé‡è¦çš„æ‰‹æ®µã€‚ æ–‡æœ¬æ‘˜è¦æ—¨åœ¨å°†æ–‡æœ¬æˆ–æ–‡æœ¬é›†åˆè½¬æ¢ä¸ºåŒ…å«å…³é”®ä¿¡æ¯çš„ç®€çŸ­æ‘˜è¦ã€‚ æ–‡æœ¬æ‘˜è¦æŒ‰ç…§è¾“å…¥ç±»å‹å¯åˆ†ä¸ºå•æ–‡æ¡£æ‘˜è¦å’Œå¤šæ–‡æ¡£æ‘˜è¦ã€‚å•æ–‡æ¡£æ‘˜è¦ä»ç»™å®šçš„ä¸€ä¸ªæ–‡æ¡£ä¸­ç”Ÿæˆæ‘˜è¦ï¼Œå¤šæ–‡æ¡£æ‘˜è¦ä»ç»™å®šçš„ä¸€ç»„ä¸»é¢˜ç›¸å…³çš„æ–‡æ¡£ä¸­ç”Ÿæˆæ‘˜è¦ã€‚ æŒ‰ç…§è¾“å‡ºç±»å‹å¯åˆ†ä¸ºæŠ½å–å¼æ‘˜è¦å’Œç”Ÿæˆå¼æ‘˜è¦ã€‚æŠ½å–å¼æ‘˜è¦ä»æºæ–‡æ¡£ä¸­æŠ½å–å…³é”®å¥å’Œå…³é”®è¯ç»„æˆæ‘˜è¦ï¼Œæ‘˜è¦å…¨éƒ¨æ¥æºäºåŸæ–‡ã€‚ç”Ÿæˆå¼æ‘˜è¦æ ¹æ®åŸæ–‡ï¼Œå…è®¸ç”Ÿæˆæ–°çš„è¯è¯­ã€çŸ­è¯­æ¥ç»„æˆæ‘˜è¦ã€‚ æŒ‰ç…§æœ‰æ— ç›‘ç£æ•°æ®å¯ä»¥åˆ†ä¸ºæœ‰ç›‘ç£æ‘˜è¦å’Œæ— ç›‘ç£æ‘˜è¦ã€‚ æœ¬æ–‡ä¸»è¦å…³æ³¨å•æ–‡æ¡£ã€æœ‰ç›‘ç£ã€æŠ½å–å¼ã€ç”Ÿæˆå¼æ‘˜è¦ã€‚\n"},{"uri":"/01introduction/100algorithm.html","title":"1.1 ç®—æ³•æ¦‚è¿°","tags":[],"description":"","content":"ä½¿ç”¨åœºæ™¯ æ ¹æ®ç”Ÿæˆæ–¹å¼å¯ä»¥åˆ†ä¸ºç”Ÿæˆå¼æ‘˜è¦å’ŒæŠ½å–å¼æ‘˜è¦ã€‚\n æŠ½å–å¼æ‘˜è¦ï¼šæ‰¾åˆ°ä¸€ä¸ªæ–‡æ¡£ä¸­æœ€é‡è¦çš„å‡ ä¸ªå¥å­å¹¶å¯¹å…¶è¿›è¡Œæ‹¼æ¥ã€‚ ç”Ÿæˆå¼æ‘˜è¦ï¼šæ˜¯ä¸€ä¸ªåºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œé€šè¿‡æºæ–‡æ¡£åºåˆ—, ç”Ÿæˆåºåˆ—æ‘˜è¦åºåˆ—  æœ¬workshopä¸»è¦è¦†ç›–ä»¥ä¸‹å‡ ä¸ªç®—æ³•ï¼Œå¯¹æ¯”ç»“æœå¦‚ä¸‹\n Pegasus BART  "},{"uri":"/01introduction/200data.html","title":"1.2 æ•°æ®é›†","tags":[],"description":"","content":"1.å…¬å¼€æ•°æ®é›† (è‹±æ–‡)  XSUM 227k BBC articles CNN/Dailymailï¼Œ93k articles from the CNN, 220k articles from the Daily Mail NEWSROOMï¼Œ1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications Multi-Newsï¼Œ56k pairs of news articles and their human-written summaries from theÂ http://sitenewser.com Gigawordï¼Œ4M examples extracted from news articlesï¼Œthe task is to generate theheadline from the first sentence arXiv, PubMedï¼Œtwo long documentdatasets of scientific publications fromÂ http://arXiv.orgÂ (113k) andPubMed (215k). The task is to generate the abstract fromthe paper body. BIGPATENTï¼Œ1.3 millionU.S. patents along with human summaries under nine patent classification categories WikiHowåœ¨çº¿http://WikiHow.comç½‘ç«™ä¸Šçš„å¤§é‡è¯´æ˜æ•°æ®é›†ã€‚ 200kç¤ºä¾‹ä¸­çš„æ¯ä¸€ä¸ªç¤ºä¾‹éƒ½åŒ…å«å¤šä¸ªæŒ‡ä»¤æ­¥éª¤æ®µè½ä»¥åŠä¸€ä¸ªæ‘˜è¦å¥å­ã€‚ ä»»åŠ¡æ˜¯ä»æ®µè½ä¸­ç”Ÿæˆä¸²è”çš„æ‘˜è¦å¥ã€‚ Reddit TIFUï¼Œ120K posts of informal stories from the online discussion forum Reddit AESLC 18k email bodies and their subjects from the Enron corpus BillSum 23k USCongressional bills and human-written reference summaries from the 103rd-115th (1993-2018) sessions of Congress.  2.å…¬å¼€æ•°æ®é›† (ä¸­æ–‡)  å“ˆå·¥å¤§çš„æ–°æµªå¾®åšçŸ­æ–‡æœ¬æ‘˜è¦ LCSTS æ•™è‚²æ–°é—»è‡ªåŠ¨æ‘˜è¦è¯­æ–™chinese_abstractive_corpus NLPCC 2017 task3 Single Document Summarization å¨±ä¹æ–°é—»ç­‰ â€œç¥ç­–æ¯â€2018é«˜æ ¡ç®—æ³•å¤§å¸ˆèµ› æ¸…å THUCNews æ€»æ•°é‡ï¼š830749ä¸ªæ ·æœ¬ï¼› æ ‡é¢˜ï¼šå¹³å‡å­—æ•° 19ï¼Œå­—æ•°æ ‡å‡†å·® 4ï¼Œæœ€å¤§å­—æ•° 48ï¼Œæœ€å°æ•°å­— 4ï¼› æ­£æ–‡ï¼šå¹³å‡å­—æ•° 892ï¼Œå­—æ•°æ ‡å‡†å·® 1012ï¼Œæœ€å¤§å­—æ•° 78796ï¼Œæœ€å°æ•°å­— 31ï¼›  "},{"uri":"/01introduction/300metrics.html","title":"1.3 è¯„ä¼°æŒ‡æ ‡","tags":[],"description":"","content":"æ–‡æœ¬ç”Ÿæˆç›®å‰çš„ä¸€å¤§ç“¶é¢ˆæ˜¯å¦‚ä½•å®¢è§‚ï¼Œå‡†ç¡®çš„è¯„ä»·æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚è‡ªåŠ¨æ–‡æ¡£æ‘˜è¦è¯„ä»·æ–¹æ³•å¤§è‡´åˆ†ä¸ºä¸¤ç±»ï¼š\nï¼ˆ1ï¼‰å†…éƒ¨è¯„ä»·æ–¹æ³•ï¼šæä¾›å‚è€ƒæ‘˜è¦ï¼Œä»¥å‚è€ƒæ‘˜è¦ä¸ºåŸºå‡†è¯„ä»·ç³»ç»Ÿæ‘˜è¦çš„è´¨é‡ã€‚ç³»ç»Ÿæ‘˜è¦ä¸å‚è€ƒæ‘˜è¦è¶Šå»åˆï¼Œè´¨é‡è¶Šé«˜ã€‚\nï¼ˆ2ï¼‰å¤–éƒ¨è¯„ä»·æ–¹æ³•ï¼šä¸æä¾›å‚è€ƒæ‘˜è¦ï¼Œåˆ©ç”¨æ–‡æ¡£æ‘˜è¦ä»£æ›¿åŸæ–‡æ¡£æ‰§è¡ŒæŸä¸ªæ–‡æ¡£ç›¸å…³çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼šæ–‡æ¡£æ£€ç´¢ã€æ–‡æ¡£åˆ†ç±»ç­‰ï¼Œèƒ½å¤Ÿæé«˜åº”ç”¨æ€§èƒ½çš„æ‘˜è¦è¢«è®¤ä¸ºæ˜¯è´¨é‡å¥½çš„æ‘˜è¦ã€‚\nä¸‹é¢ä»‹ç»ä¸¤ç§æ¯”è¾ƒç®€å•çš„ï¼Œç»å¸¸ç”¨åˆ°çš„å†…éƒ¨è¯„ä»·æ–¹æ³•ï¼š\nEdmundson é€‚äºæŠ½å–å¼æ–‡æœ¬æ‘˜è¦ï¼Œæ¯”è¾ƒæœºæ¢°æ–‡æ‘˜(è‡ªåŠ¨æ–‡æ‘˜ç³»ç»Ÿå¾—åˆ°çš„æ–‡æ‘˜)ä¸ç›®æ ‡æ–‡æ‘˜(ä»åŸæ–‡ä¸­æŠ½å–çš„å¥å­)çš„å¥å­é‡åˆç‡çš„é«˜ä½å¯¹ç³»ç»Ÿæ‘˜è¦è¿›è¡Œè¯„ä»·ã€‚\nè®¡ç®—å…¬å¼ï¼š\n é‡åˆç‡p = åŒ¹é…å¥å­æ•°/ä¸“å®¶æ–‡æ‘˜å¥å­æ•°*100%  æ¯ä¸€ä¸ªæœºæ¢°æ–‡æ‘˜çš„é‡åˆç‡ä¸ºæŒ‰ä¸‰ä¸ªä¸“å®¶ç»™å‡ºçš„æ–‡æ‘˜å¾—åˆ°çš„é‡åˆç‡çš„å¹³å‡å€¼ï¼ˆå…¶ä¸­ï¼Œpiä¸ºç›¸å¯¹äºç¬¬iä¸ªä¸“å®¶çš„é‡åˆç‡ï¼Œnä¸ºä¸“å®¶æ–‡æ‘˜æ€»æ•°ï¼‰ï¼š ROUGE ROUGEï¼ˆRecall-Oriented Understudy for Gisting Evaluationï¼‰åŸºäºæ‘˜è¦ä¸­n-gramçš„å…±ç°ä¿¡æ¯è¯„ä»·æ‘˜è¦ï¼Œæ˜¯ä¸€ç§é¢å‘nå…ƒè¯å¬å›ç‡çš„è¯„ä»·æ–¹æ³•ã€‚ å…¶ä¸­ï¼ŒRef summariesè¡¨ç¤ºæ ‡å‡†æ‘˜è¦ï¼Œcount_match(n-gram)è¡¨ç¤ºç”Ÿæˆæ‘˜è¦å’Œæ ‡å‡†æ‘˜è¦ä¸­åŒæ—¶å‡ºç°n-gramçš„ä¸ªæ•°ï¼Œcount(n-gram)è¡¨ç¤ºå‚è€ƒæ‘˜è¦ä¸­å‡ºç°çš„n-gramä¸ªæ•°ã€‚\n"},{"uri":"/02extraction.html","title":"åŠ¨æ‰‹å®éªŒ1: åŸºäºAmazon SageMakerè®­ç»ƒä¸€ä¸ªç«¯åˆ°ç«¯æŠ½å–å¼çš„ABSAæ¨¡å‹","tags":[],"description":"","content":"paper - A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extractionã€‚\nSOTA for ate taskï¼š https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval?p=a-multi-task-learning-model-for-chinese æˆ‘ä»¬ç°åœ¨ä¼šç”¨sagemakerè¿›è¡Œä¸€ä¸ªæ¨¡å‹çš„æœ¬åœ°è®­ç»ƒï¼Œä½¿ç”¨ml.p3.8xlargeæœºå‹ã€‚\næ•°æ®å‡†å¤‡ é¦–å…ˆä¸‹è½½ä»£ç \nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/PyABSA.git å°†æ•°æ®data1119.csvä¸Šä¼ åˆ°PyABSA/data/data1109.csv\næ¨¡å‹è®­ç»ƒ æ•°æ®å‡†å¤‡\ncd PyABSA pip install termcolor update_checker findfile jupyterlab-git torch==1.10.0 transformers==4.12.3 autocuda spacy googledrivedownloader seqeval emoji python pyabsa/utils/preprocess.py --inpath './data/data1109.csv' --folder_name 'custom_atepc_1109' --task 'aptepc' ç„¶åè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ¼”ç¤ºç›®çš„ï¼Œåªè®­ç»ƒä¸€ä¸ªepoch\nfrom pyabsa.functional import ATEPCModelList from pyabsa.functional import Trainer, ATEPCTrainer from pyabsa.functional import ATEPCConfigManager atepc_config_custom = ATEPCConfigManager.get_atepc_config_chinese() atepc_config_custom.num_epoch = 2 atepc_config_custom.evaluate_begin = 1 atepc_config_custom.log_step = 100 atepc_config_custom.model = ATEPCModelList.LCF_ATEPC aspect_extractor = ATEPCTrainer(config=atepc_config_custom, dataset=\u0026#39;./custom_atepc_1109\u0026#39; ) è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹è¯„ä¼°\npython utils/metrics_cacl.py --data_path --checkppoint "},{"uri":"/02extraction/01train.html","title":"æŠ½å–å¼ABSAæ¨¡å‹è®­ç»ƒ1","tags":[],"description":"","content":"paper - A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extractionã€‚\nSOTA for ate taskï¼š https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval?p=a-multi-task-learning-model-for-chinese æˆ‘ä»¬ç°åœ¨ä¼šç”¨sagemakerè¿›è¡Œä¸€ä¸ªæ¨¡å‹çš„æœ¬åœ°è®­ç»ƒï¼Œä½¿ç”¨ml.p3.8xlargeæœºå‹ã€‚\næ•°æ®å‡†å¤‡ é¦–å…ˆä¸‹è½½ä»£ç \nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/PyABSA.git å°†æ•°æ®data1119.csvä¸Šä¼ åˆ°PyABSA/data/data1109.csv\næ¨¡å‹è®­ç»ƒ æ•°æ®å‡†å¤‡\ncd PyABSA pip install termcolor update_checker findfile jupyterlab-git torch==1.10.0 transformers==4.12.3 autocuda spacy googledrivedownloader seqeval emoji python pyabsa/utils/preprocess.py --inpath './data/data1109.csv' --folder_name 'custom_atepc_1109' --task 'aptepc' ç„¶åè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ¼”ç¤ºç›®çš„ï¼Œåªè®­ç»ƒä¸€ä¸ªepoch\nfrom pyabsa.functional import ATEPCModelList from pyabsa.functional import Trainer, ATEPCTrainer from pyabsa.functional import ATEPCConfigManager atepc_config_custom = ATEPCConfigManager.get_atepc_config_chinese() atepc_config_custom.num_epoch = 2 atepc_config_custom.evaluate_begin = 1 atepc_config_custom.log_step = 100 atepc_config_custom.model = ATEPCModelList.LCF_ATEPC aspect_extractor = ATEPCTrainer(config=atepc_config_custom, dataset=\u0026#39;./custom_atepc_1109\u0026#39; ) è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹è¯„ä¼°\npython utils/metrics_cacl.py --data_path --checkppoint "},{"uri":"/03generative/0501train.html","title":"ç”Ÿæˆå¼çš„ABSAæ¨¡å‹è®­ç»ƒ","tags":[],"description":"","content":"æˆ‘ä»¬ç°åœ¨ä¼šç”¨sagemakerè¿›è¡Œä¸€ä¸ªç”Ÿæˆå¼çš„ABSAæ¨¡å‹çš„æœ¬åœ°è®­ç»ƒï¼Œä½¿ç”¨ML.P3.2xlargeæœºå‹ã€‚\næ•°æ®å‡†å¤‡ é¦–å…ˆä¸‹è½½ä»£ç \ncd SageMaker git clone https://github.com/HaoranLv/Generative-ABSA-SageMaker.git å®‰è£…ç¯å¢ƒ\nsource activate pytorch_p37 pip install -r requirements.txt ç„¶åå¤„ç†æ•°æ®data_prepare.ipynbï¼Œè¿›è¡Œæ•°æ®æ¸…æ´—å¹¶åˆ‡åˆ†train/testã€‚ æ³¨æ„è¿™é‡Œï¼Œä¸ºäº†å¿«é€Ÿäº§ç”Ÿç»“æœï¼Œæˆ‘ä»¬åªè¦ç”¨800æ¡æ•°æ®è®­ç»ƒï¼Œ200æ¡æµ‹è¯•/éªŒè¯\ndf=pd.read_csv('data/ctrip/data1119_part.csv') write_txt(df,path='data/ctrip/total.txt') write_train_test(train_path='data/ctrip/train.txt',test_path='data/ctrip/test.txt',root='data/ctrip/total1119.txt') æ¨¡å‹è®­ç»ƒ æ¥ä¸‹æ¥æˆ‘ä»¬è¿è¡Œè®­ç»ƒ,è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨huggingfaceä¸Šçš„å…¬å¼€çš„lemon234071/t5-base-Chineseä½œä¸ºè®­ç»ƒèµ·ç‚¹ï¼Œä¸ºäº†æ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬åªè¿è¡Œä¸€ä¸ªepochï¼Œå¤§çº¦éœ€è¦5min\npython -u main.py --task tasd-cn \\ --dataset ctrip \\ --paradigm extraction \\ --n_gpu '0' \\ --model_name_or_path lemon234071/t5-base-Chinese \\ --do_train \\ --train_batch_size 2 \\ --gradient_accumulation_steps 2 \\ --eval_batch_size 2 \\ --learning_rate 3e-4 \\ --num_train_epochs 25 \u0026gt; logs/noemj_lr3e-4.log è®­ç»ƒå®Œæˆåï¼Œä¼šæç¤ºæ—¥å¿—ä¿¡æ¯å¦‚ä¸‹\nFinish training and saving the model! æ¨¡å‹ç»“æœæ–‡ä»¶åŠç›¸åº”çš„æ—¥å¿—ç­‰ä¿¡æ¯ä¼šè‡ªåŠ¨ä¿å­˜åœ¨./outputs/tasd-cn/ctrip/extraction/\nç»“æœæœ¬åœ°éªŒè¯ æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨è¿™ä¸ªäº§ç”Ÿçš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œæœ¬åœ°æ¨ç†ã€‚æ³¨æ„è¿™é‡Œçš„æ¨¡å‹æ–‡ä»¶åœ°å€çš„æŒ‡å®šä¸ºä½ åˆšåˆšè®­ç»ƒäº§ç”Ÿçš„ã€‚\npython main.py --task tasd-cn \\ --dataset ctrip \\ --ckpoint_path outputs/tasd-cn/ctrip/extraction/cktepoch=1.ckpt \\ --paradigm extraction \\ --n_gpu '0' \\ --do_direct_eval \\ --eval_batch_size 128 \\ ç»“æœæœ¬åœ°æµ‹è¯• æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨è¿™ä¸ªäº§ç”Ÿçš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œæœ¬åœ°æ¨ç†ã€‚æ³¨æ„è¿™é‡Œçš„æ¨¡å‹æ–‡ä»¶åœ°å€çš„æŒ‡å®šä¸ºä½ åˆšåˆšè®­ç»ƒäº§ç”Ÿçš„ã€‚\npython main.py --task tasd-cn \\ --dataset ctrip \\ --ckpoint_path outputs/tasd-cn/ctrip/extraction/cktepoch=1.ckpt \\ --text æ—©é¤ä¸€èˆ¬èˆ¬ï¼Œå‹‰å‹‰å¼ºå¼ºå¡«é¥±è‚šå­ï¼Œæ ·å¼å¯é€‰æ€§ä¸å¤šï¼Œå¯èƒ½æ˜¯ç–«æƒ…çš„å½±å“å§ã€‚ä¸è¿‡é…’åº—çš„æœåŠ¡ä¸é”™ï¼Œäº”ä¸ªå°å­©æ—©é¤éƒ½é€äº†ï¼Œç‚¹ğŸ‘ã€‚ç”±äºé…’åº—å†å²æœ‰ç‚¹é•¿ï¼Œæ‰€ä»¥è®¾æ–½æ„Ÿè§‰ä¸€èˆ¬èˆ¬ï¼Œæ•´ä½“è¿˜å¯ä»¥ï¼Œä¸‰é’»å§ \\ --paradigm extraction \\ --n_gpu 0 \\ --do_direct_predict \\ è¾“å‡ºå¦‚ä¸‹\nåŸæ–‡: Germany on Wednesday accused Vietnam of kidnapping a former Vietnamese oil executive Trinh Xuan Thanh, who allegedly sought asylum in Berlin, and taking him home to face accusations of corruption. Germany expelled a Vietnamese intelligence officer over the suspected kidnapping and demanded that Vietnam allow Thanh to return to Germany. However, Vietnam said Thanh had returned home by himself. çœŸå®æ ‡ç­¾: Germany accuses Vietnam of kidnapping asylum seeker æ¨¡å‹é¢„æµ‹: Germany accuses Vietnam of kidnapping ex-oil exec, taking him home åˆ°è¿™é‡Œï¼Œå°±å®Œæˆäº†ä¸€ä¸ªæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚\n"},{"uri":"/02extraction/02train.html","title":"æŠ½å–å¼ABSAæ¨¡å‹è®­ç»ƒ2","tags":[],"description":"","content":"paper - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\u0026quot; (NAACL 2019)ã€‚ è¿™ä¸ªæ–¹æ³•çš„è¾“å…¥è¾“å‡ºè®¾è®¡å¾ˆç¬¦åˆTABSAä»»åŠ¡ï¼Œåšæ³•ç®€å•ï¼Œé€‚åˆä½œä¸ºæ­¤ç±»ä»»åŠ¡çš„baseline.\næˆ‘ä»¬ç°åœ¨ä¼šç”¨sagemakerè¿›è¡Œä¸€ä¸ªæ¨¡å‹çš„æœ¬åœ°è®­ç»ƒï¼Œä½¿ç”¨ml.p3.8xlargeæœºå‹ã€‚\næ•°æ®å‡†å¤‡ é¦–å…ˆä¸‹è½½ä»£ç \nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/ABSA-BERT-pair.git å°†æ•°æ®data1119.csvä¸Šä¼ åˆ°data/custom/data1119.csv\næ¨¡å‹è®­ç»ƒ æ¥ä¸‹æ¥æˆ‘ä»¬è¿è¡Œè®­ç»ƒï¼Œé¦–å…ˆä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è½¬æ¢ä¸ºtorchç‰ˆæœ¬\ncd ABSA-BERT-pair wget -P ./source/bert/pretrain_model/cn https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip cd ./source/bert/pretrain_model/cn unzip chinese_L-12_H-768_A-12.zip pip install tensorflow==1.13.1 cd /home/ec2-user/SageMaker/ABSA-BERT-pair/ python convert_tf_checkpoint_to_pytorch.py \\ --tf_checkpoint_path ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_model.ckpt \\ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \\ --pytorch_dump_path ./source/bert/pretrain_model/cn/pytorch_model.bin æ•°æ®å‡†å¤‡\ncd generate/ python generate_custom_NLI_M.py ç„¶åè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ¼”ç¤ºç›®çš„ï¼Œåªè®­ç»ƒä¸€ä¸ªepoch\ncd ../ CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_TABSA-v1.py \\ --task_name custom_NLI_M \\ --data_dir data/custom/bert-pair/ \\ --vocab_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/vocab.txt \\ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \\ --init_checkpoint ./source/bert/pretrain_model/cn/pytorch_model.bin \\ --eval_test \\ --do_lower_case \\ --max_seq_length 512 \\ --train_batch_size 48 \\ --learning_rate 2e-5 \\ --num_train_epochs 1.0 \\ --do_save_model \\ --output_dir results/custom/NLI_M \\ --seed 42 è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹è¯„ä¼°\npython evaluation.py --task_name custom_NLI_M --pred_data_dir results/custom/NLI_M/test_ep_1.txt "},{"uri":"/03generative.html","title":"åŠ¨æ‰‹å®éªŒ2: åŸºäºAmazon SageMakerè®­ç»ƒä¸€ä¸ªç«¯åˆ°ç«¯ç”Ÿæˆå¼çš„ABSAæ¨¡å‹ä»¥åŠéƒ¨ç½²","tags":[],"description":"","content":"æ¨¡å‹æ¶æ„ Pegasusæ˜¯ä¸€ä¸ªæ ‡å‡†çš„Transformeræ¶æ„ï¼Œå…¶å…¨ç§°æ˜¯ï¼šåˆ©ç”¨æå–çš„é—´éš™å¥è¿›è¡Œæ‘˜è¦æ¦‚æ‹¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPre-training with Extracted Gap-sentences for Abstractive Summarizationï¼‰ã€‚å°±æ˜¯è®¾è®¡ä¸€ç§é—´éš™å¥ç”Ÿæˆçš„è‡ªç›‘ç£é¢„è®­ç»ƒç›®æ ‡ï¼ˆGSGï¼‰ï¼Œæ¥æ”¹è¿›ç”Ÿæˆæ‘˜è¦çš„å¾®è°ƒæ€§èƒ½ã€‚\nGSGé¢„è®­ç›®æ ‡  Maskæ‰æ•´ä¸ªå¥å­ å°†Maskçš„å¥å­æ‹¼æ¥æˆæ‘˜è¦ ä½¿ç”¨ä¸Šä¸‹æ–‡è¡¥å…¨  Maskç­–ç•¥\næ¨¡å‹è¡¨ç° Pegasus-largeåœ¨å„ä¸ªæ‘˜è¦æ•°æ®é›†ä¸Šçš„ç»“æœåŒä¹‹å‰çš„SOTAå¯¹æ¯”ï¼šåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡å¾—åˆ°äº†SOTAçš„æ€§èƒ½è¡¨ç°ã€‚\nreference  paperï¼š https://arxiv.org/abs/1912.08777 source codeï¼šhttps://github.com/google-research/pegasus  @misc{zhang2019pegasus, title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu}, year={2020}, eprint={1912.08777}, archivePrefix={ICML}, primaryClass={cs.CL} }\n"},{"uri":"/03generative/0503deploy.html","title":"ç”Ÿæˆå¼ABSAæ¨¡å‹éƒ¨ç½²","tags":[],"description":"","content":"é¦–å…ˆæ‰“åŒ…é•œåƒå¹¶æ¨é€\nsh endpoint/build_and_push.sh generative-absa ç„¶åéƒ¨ç½²ä¸€ä¸ªé¢„ç½®çš„endpoint\n#æ³¨æ„ä¿®æ”¹ï¼š847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absaä¸ºè‡ªå·±å¯¹åº”çš„ !python endpoint/create_endpoint.py \\ --endpoint_ecr_image_path \u0026#34;847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absa\u0026#34; \\ --endpoint_name \u0026#39;generative-absa\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; è¾“å‡º\nmodel_name: generative-absa endpoint_ecr_image_path: 847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absa \u0026lt;\u0026lt;\u0026lt; Completed model endpoint deployment. generative-absa å½“çŠ¶æ€å˜ä¸ºInServiceå³ä»£è¡¨éƒ¨ç½²å®Œæˆ\nåœ¨éƒ¨ç½²ç»“æŸåï¼Œçœ‹åˆ°SageMakeræ§åˆ¶å°ç”Ÿæˆäº†å¯¹åº”çš„endpoint,å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å®¢æˆ·ç«¯ä»£ç æµ‹è¯•è°ƒç”¨\n%%time from boto3.session import Session import json data={\u0026#34;data\u0026#34;: \u0026#39;æ—©é¤ä¸€èˆ¬èˆ¬ï¼Œå‹‰å‹‰å¼ºå¼ºå¡«é¥±è‚šå­ï¼Œæ ·å¼å¯é€‰æ€§ä¸å¤šï¼Œå¯èƒ½æ˜¯ç–«æƒ…çš„å½±å“å§ã€‚ä¸è¿‡é…’åº—çš„æœåŠ¡ä¸é”™ï¼Œäº”ä¸ªå°å­©æ—©é¤éƒ½é€äº†ï¼Œç‚¹ğŸ‘ã€‚ç”±äºé…’åº—å†å²æœ‰ç‚¹é•¿ï¼Œæ‰€ä»¥è®¾æ–½æ„Ÿè§‰ä¸€èˆ¬èˆ¬ï¼Œæ•´ä½“è¿˜å¯ä»¥ï¼Œä¸‰é’»å§\u0026#39;} session = Session() runtime = session.client(\u0026#34;runtime.sagemaker\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;absa\u0026#39;, ContentType=\u0026#34;application/json\u0026#34;, Body=json.dumps(data), ) result = json.loads(response[\u0026#34;Body\u0026#34;].read()) print (result) ç»“æœå¦‚ä¸‹\n{'result': '(å°å­©æ—©é¤, å„¿ç«¥é¤é¥®, é€äº†ç‚¹ğŸ‘, æ­£)', 'infer_time': '0:00:00.725859'} "},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]