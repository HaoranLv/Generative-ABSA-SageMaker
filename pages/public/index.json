[{"uri":"/","title":"AWS Datalab-细粒度情感分析","tags":[],"description":"","content":"Author\n JUNYI LIU (AWS GCR Applied Scientist)  概述 本次workshop分为几个部分\n 背景介绍- what is text summary？ 基于Amazon SageMaker的TEXTRANK模型训练动手实验 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型  模型训练ß 模型部署   基于Amazon SageMaker的MT5模型训练动手实验  本地模型训练 启动模型训练任务 模型部署   基于Amazon SageMaker的CPT模型训练动手实验  模型训练 模型增强训练 模型部署    本次 workshop 前提 本次 workshop 建议在 US-WEST-2 Region 使用。为了演示方便，所以本 workshop 所有的演示都会以US-WEST-2 Region 为例。\n"},{"uri":"/01introduction.html","title":"Introduction","tags":[],"description":"","content":"text summary（文档摘要） 随着互联网产生的文本数据越来越多，文本信息过载问题日益严重，对各类文本进行一个“降 维”处理显得非常必要，文本摘要便是其中一个重要的手段。 文本摘要旨在将文本或文本集合转换为包含关键信息的简短摘要。 文本摘要按照输入类型可分为单文档摘要和多文档摘要。单文档摘要从给定的一个文档中生成摘要，多文档摘要从给定的一组主题相关的文档中生成摘要。 按照输出类型可分为抽取式摘要和生成式摘要。抽取式摘要从源文档中抽取关键句和关键词组成摘要，摘要全部来源于原文。生成式摘要根据原文，允许生成新的词语、短语来组成摘要。 按照有无监督数据可以分为有监督摘要和无监督摘要。 本文主要关注单文档、有监督、抽取式、生成式摘要。\n"},{"uri":"/01introduction/100algorithm.html","title":"1.1 算法概述","tags":[],"description":"","content":"使用场景 根据生成方式可以分为生成式摘要和抽取式摘要。\n 抽取式摘要：找到一个文档中最重要的几个句子并对其进行拼接。 生成式摘要：是一个序列生成问题，通过源文档序列, 生成序列摘要序列  本workshop主要覆盖以下几个算法，对比结果如下\n Pegasus BART  "},{"uri":"/01introduction/200data.html","title":"1.2 数据集","tags":[],"description":"","content":"1.公开数据集 (英文)  XSUM 227k BBC articles CNN/Dailymail，93k articles from the CNN, 220k articles from the Daily Mail NEWSROOM，1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications Multi-News，56k pairs of news articles and their human-written summaries from the http://sitenewser.com Gigaword，4M examples extracted from news articles，the task is to generate theheadline from the first sentence arXiv, PubMed，two long documentdatasets of scientific publications from http://arXiv.org (113k) andPubMed (215k). The task is to generate the abstract fromthe paper body. BIGPATENT，1.3 millionU.S. patents along with human summaries under nine patent classification categories WikiHow在线http://WikiHow.com网站上的大量说明数据集。 200k示例中的每一个示例都包含多个指令步骤段落以及一个摘要句子。 任务是从段落中生成串联的摘要句。 Reddit TIFU，120K posts of informal stories from the online discussion forum Reddit AESLC 18k email bodies and their subjects from the Enron corpus BillSum 23k USCongressional bills and human-written reference summaries from the 103rd-115th (1993-2018) sessions of Congress.  2.公开数据集 (中文)  哈工大的新浪微博短文本摘要 LCSTS 教育新闻自动摘要语料chinese_abstractive_corpus NLPCC 2017 task3 Single Document Summarization 娱乐新闻等 “神策杯”2018高校算法大师赛 清华 THUCNews 总数量：830749个样本； 标题：平均字数 19，字数标准差 4，最大字数 48，最小数字 4； 正文：平均字数 892，字数标准差 1012，最大字数 78796，最小数字 31；  "},{"uri":"/01introduction/300metrics.html","title":"1.3 评估指标","tags":[],"description":"","content":"文本生成目前的一大瓶颈是如何客观，准确的评价机器生成文本的质量。自动文档摘要评价方法大致分为两类：\n（1）内部评价方法：提供参考摘要，以参考摘要为基准评价系统摘要的质量。系统摘要与参考摘要越吻合，质量越高。\n（2）外部评价方法：不提供参考摘要，利用文档摘要代替原文档执行某个文档相关的应用。例如：文档检索、文档分类等，能够提高应用性能的摘要被认为是质量好的摘要。\n下面介绍两种比较简单的，经常用到的内部评价方法：\nEdmundson 适于抽取式文本摘要，比较机械文摘(自动文摘系统得到的文摘)与目标文摘(从原文中抽取的句子)的句子重合率的高低对系统摘要进行评价。\n计算公式：\n 重合率p = 匹配句子数/专家文摘句子数*100%  每一个机械文摘的重合率为按三个专家给出的文摘得到的重合率的平均值（其中，pi为相对于第i个专家的重合率，n为专家文摘总数）： ROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）基于摘要中n-gram的共现信息评价摘要，是一种面向n元词召回率的评价方法。 其中，Ref summaries表示标准摘要，count_match(n-gram)表示生成摘要和标准摘要中同时出现n-gram的个数，count(n-gram)表示参考摘要中出现的n-gram个数。\n"},{"uri":"/02extraction.html","title":"动手实验1: 基于Amazon SageMaker训练一个端到端抽取式的ABSA模型","tags":[],"description":"","content":"paper - A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction。\nSOTA for ate task： https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval?p=a-multi-task-learning-model-for-chinese 我们现在会用sagemaker进行一个模型的本地训练，使用ml.p3.8xlarge机型。\n数据准备 首先下载代码\nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/PyABSA.git 将数据data1119.csv上传到PyABSA/data/data1109.csv\n模型训练 数据准备\ncd PyABSA pip install termcolor update_checker findfile jupyterlab-git torch==1.10.0 transformers==4.12.3 autocuda spacy googledrivedownloader seqeval emoji python pyabsa/utils/preprocess.py --inpath './data/data1109.csv' --folder_name 'custom_atepc_1109' --task 'aptepc' 然后进行模型训练，演示目的，只训练一个epoch\nfrom pyabsa.functional import ATEPCModelList from pyabsa.functional import Trainer, ATEPCTrainer from pyabsa.functional import ATEPCConfigManager atepc_config_custom = ATEPCConfigManager.get_atepc_config_chinese() atepc_config_custom.num_epoch = 2 atepc_config_custom.evaluate_begin = 1 atepc_config_custom.log_step = 100 atepc_config_custom.model = ATEPCModelList.LCF_ATEPC aspect_extractor = ATEPCTrainer(config=atepc_config_custom, dataset=\u0026#39;./custom_atepc_1109\u0026#39; ) 训练完成后，模型评估\npython utils/metrics_cacl.py --data_path --checkppoint "},{"uri":"/02extraction/01train.html","title":"抽取式ABSA模型训练1","tags":[],"description":"","content":"paper - A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction。\nSOTA for ate task： https://paperswithcode.com/sota/aspect-based-sentiment-analysis-on-semeval?p=a-multi-task-learning-model-for-chinese 我们现在会用sagemaker进行一个模型的本地训练，使用ml.p3.8xlarge机型。\n数据准备 首先下载代码\nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/PyABSA.git 将数据data1119.csv上传到PyABSA/data/data1109.csv\n模型训练 数据准备\ncd PyABSA pip install termcolor update_checker findfile jupyterlab-git torch==1.10.0 transformers==4.12.3 autocuda spacy googledrivedownloader seqeval emoji python pyabsa/utils/preprocess.py --inpath './data/data1109.csv' --folder_name 'custom_atepc_1109' --task 'aptepc' 然后进行模型训练，演示目的，只训练一个epoch\nfrom pyabsa.functional import ATEPCModelList from pyabsa.functional import Trainer, ATEPCTrainer from pyabsa.functional import ATEPCConfigManager atepc_config_custom = ATEPCConfigManager.get_atepc_config_chinese() atepc_config_custom.num_epoch = 2 atepc_config_custom.evaluate_begin = 1 atepc_config_custom.log_step = 100 atepc_config_custom.model = ATEPCModelList.LCF_ATEPC aspect_extractor = ATEPCTrainer(config=atepc_config_custom, dataset=\u0026#39;./custom_atepc_1109\u0026#39; ) 训练完成后，模型评估\npython utils/metrics_cacl.py --data_path --checkppoint "},{"uri":"/03generative/0501train.html","title":"生成式的ABSA模型训练","tags":[],"description":"","content":"我们现在会用sagemaker进行一个生成式的ABSA模型的本地训练，使用ML.P3.2xlarge机型。\n数据准备 首先下载代码\ncd SageMaker git clone https://github.com/HaoranLv/Generative-ABSA-SageMaker.git 安装环境\nsource activate pytorch_p37 pip install -r requirements.txt 然后处理数据data_prepare.ipynb，进行数据清洗并切分train/test。 注意这里，为了快速产生结果，我们只要用800条数据训练，200条测试/验证\ndf=pd.read_csv('data/ctrip/data1119_part.csv') write_txt(df,path='data/ctrip/total.txt') write_train_test(train_path='data/ctrip/train.txt',test_path='data/ctrip/test.txt',root='data/ctrip/total1119.txt') 模型训练 接下来我们运行训练,这里我们使用huggingface上的公开的lemon234071/t5-base-Chinese作为训练起点，为了演示目的，我们只运行一个epoch，大约需要5min\npython -u main.py --task tasd-cn \\ --dataset ctrip \\ --paradigm extraction \\ --n_gpu '0' \\ --model_name_or_path lemon234071/t5-base-Chinese \\ --do_train \\ --train_batch_size 2 \\ --gradient_accumulation_steps 2 \\ --eval_batch_size 2 \\ --learning_rate 3e-4 \\ --num_train_epochs 25 \u0026gt; logs/noemj_lr3e-4.log 训练完成后，会提示日志信息如下\nFinish training and saving the model! 模型结果文件及相应的日志等信息会自动保存在./outputs/tasd-cn/ctrip/extraction/\n结果本地验证 我们可以直接用这个产生的模型文件进行本地推理。注意这里的模型文件地址的指定为你刚刚训练产生的。\npython main.py --task tasd-cn \\ --dataset ctrip \\ --ckpoint_path outputs/tasd-cn/ctrip/extraction/cktepoch=1.ckpt \\ --paradigm extraction \\ --n_gpu '0' \\ --do_direct_eval \\ --eval_batch_size 128 \\ 结果本地测试 我们可以直接用这个产生的模型文件进行本地推理。注意这里的模型文件地址的指定为你刚刚训练产生的。\npython main.py --task tasd-cn \\ --dataset ctrip \\ --ckpoint_path outputs/tasd-cn/ctrip/extraction/cktepoch=1.ckpt \\ --text 早餐一般般，勉勉强强填饱肚子，样式可选性不多，可能是疫情的影响吧。不过酒店的服务不错，五个小孩早餐都送了，点👍。由于酒店历史有点长，所以设施感觉一般般，整体还可以，三钻吧 \\ --paradigm extraction \\ --n_gpu 0 \\ --do_direct_predict \\ 输出如下\n原文: Germany on Wednesday accused Vietnam of kidnapping a former Vietnamese oil executive Trinh Xuan Thanh, who allegedly sought asylum in Berlin, and taking him home to face accusations of corruption. Germany expelled a Vietnamese intelligence officer over the suspected kidnapping and demanded that Vietnam allow Thanh to return to Germany. However, Vietnam said Thanh had returned home by himself. 真实标签: Germany accuses Vietnam of kidnapping asylum seeker 模型预测: Germany accuses Vietnam of kidnapping ex-oil exec, taking him home 到这里，就完成了一个模型的训练过程。\n"},{"uri":"/02extraction/02train.html","title":"抽取式ABSA模型训练2","tags":[],"description":"","content":"paper - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\u0026quot; (NAACL 2019)。 这个方法的输入输出设计很符合TABSA任务，做法简单，适合作为此类任务的baseline.\n我们现在会用sagemaker进行一个模型的本地训练，使用ml.p3.8xlarge机型。\n数据准备 首先下载代码\nsource activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/ABSA-BERT-pair.git 将数据data1119.csv上传到data/custom/data1119.csv\n模型训练 接下来我们运行训练，首先下载预训练模型并转换为torch版本\ncd ABSA-BERT-pair wget -P ./source/bert/pretrain_model/cn https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip cd ./source/bert/pretrain_model/cn unzip chinese_L-12_H-768_A-12.zip pip install tensorflow==1.13.1 cd /home/ec2-user/SageMaker/ABSA-BERT-pair/ python convert_tf_checkpoint_to_pytorch.py \\ --tf_checkpoint_path ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_model.ckpt \\ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \\ --pytorch_dump_path ./source/bert/pretrain_model/cn/pytorch_model.bin 数据准备\ncd generate/ python generate_custom_NLI_M.py 然后进行模型训练，演示目的，只训练一个epoch\ncd ../ CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_TABSA-v1.py \\ --task_name custom_NLI_M \\ --data_dir data/custom/bert-pair/ \\ --vocab_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/vocab.txt \\ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \\ --init_checkpoint ./source/bert/pretrain_model/cn/pytorch_model.bin \\ --eval_test \\ --do_lower_case \\ --max_seq_length 512 \\ --train_batch_size 48 \\ --learning_rate 2e-5 \\ --num_train_epochs 1.0 \\ --do_save_model \\ --output_dir results/custom/NLI_M \\ --seed 42 训练完成后，模型评估\npython evaluation.py --task_name custom_NLI_M --pred_data_dir results/custom/NLI_M/test_ep_1.txt "},{"uri":"/03generative.html","title":"动手实验2: 基于Amazon SageMaker训练一个端到端生成式的ABSA模型以及部署","tags":[],"description":"","content":"模型架构 Pegasus是一个标准的Transformer架构，其全称是：利用提取的间隙句进行摘要概括的预训练模型（Pre-training with Extracted Gap-sentences for Abstractive Summarization）。就是设计一种间隙句生成的自监督预训练目标（GSG），来改进生成摘要的微调性能。\nGSG预训目标  Mask掉整个句子 将Mask的句子拼接成摘要 使用上下文补全  Mask策略\n模型表现 Pegasus-large在各个摘要数据集上的结果同之前的SOTA对比：在所有数据集上均得到了SOTA的性能表现。\nreference  paper： https://arxiv.org/abs/1912.08777 source code：https://github.com/google-research/pegasus  @misc{zhang2019pegasus, title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu}, year={2020}, eprint={1912.08777}, archivePrefix={ICML}, primaryClass={cs.CL} }\n"},{"uri":"/03generative/0503deploy.html","title":"生成式ABSA模型部署","tags":[],"description":"","content":"首先打包镜像并推送\nsh endpoint/build_and_push.sh generative-absa 然后部署一个预置的endpoint\n#注意修改：847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absa为自己对应的 !python endpoint/create_endpoint.py \\ --endpoint_ecr_image_path \u0026#34;847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absa\u0026#34; \\ --endpoint_name \u0026#39;generative-absa\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; 输出\nmodel_name: generative-absa endpoint_ecr_image_path: 847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/generative-absa \u0026lt;\u0026lt;\u0026lt; Completed model endpoint deployment. generative-absa 当状态变为InService即代表部署完成\n在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\n%%time from boto3.session import Session import json data={\u0026#34;data\u0026#34;: \u0026#39;早餐一般般，勉勉强强填饱肚子，样式可选性不多，可能是疫情的影响吧。不过酒店的服务不错，五个小孩早餐都送了，点👍。由于酒店历史有点长，所以设施感觉一般般，整体还可以，三钻吧\u0026#39;} session = Session() runtime = session.client(\u0026#34;runtime.sagemaker\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;absa\u0026#39;, ContentType=\u0026#34;application/json\u0026#34;, Body=json.dumps(data), ) result = json.loads(response[\u0026#34;Body\u0026#34;].read()) print (result) 结果如下\n{'result': '(小孩早餐, 儿童餐饮, 送了点👍, 正)', 'infer_time': '0:00:00.725859'} "},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]