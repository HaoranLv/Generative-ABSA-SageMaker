import argparse
import os
import logging
import time
import pickle
from tqdm import tqdm

import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning import seed_everything

from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer
from transformers import get_linear_schedule_with_warmup

from datasets_utils.data_utils import ABSADataset
from datasets_utils.data_utils import write_results_to_log, read_line_examples_from_file
from eval_utils import compute_scores


logger = logging.getLogger(__name__)


def init_args():
    parser = argparse.ArgumentParser()
    # basic settings
    parser.add_argument("--task", default='uabsa', type=str, required=True,
                        help="The name of the task, selected from: [uabsa, aste, tasd, aope]")
    parser.add_argument("--dataset", default='rest14', type=str, required=True,
                        help="The name of the dataset, selected from: [laptop14, rest14, rest15, rest16]")
    parser.add_argument("--model_name_or_path", default='lemon234071/t5-base-Chinese', type=str,
                        help="Path to pre-trained model or shortcut name")
    parser.add_argument("--paradigm", default='annotation', type=str, required=True,
                        help="The way to construct target sentence, selected from: [annotation, extraction]")
    parser.add_argument("--do_train", action='store_true', help="Whether to run training.")
    parser.add_argument("--do_eval", action='store_true', help="Whether to run eval on the dev/test set.")
    parser.add_argument("--do_direct_eval", action='store_true', 
                        help="Whether to run direct eval on the dev/test set.")
    parser.add_argument("--do_direct_predict", action='store_true', 
                        help="Whether to run direct eval on the dev/test set.")

    # Other parameters
    parser.add_argument("--max_seq_length", default=128, type=int)
    parser.add_argument("--n_gpu", default=0)
    parser.add_argument("--train_batch_size", default=16, type=int,
                        help="Batch size per GPU/CPU for training.")
    parser.add_argument("--eval_batch_size", default=16, type=int,
                        help="Batch size per GPU/CPU for evaluation.")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    parser.add_argument("--learning_rate", default=3e-4, type=float)
    parser.add_argument("--num_train_epochs", default=20, type=int, 
                        help="Total number of training epochs to perform.")
    parser.add_argument('--seed', type=int, default=42, help="random seed for initialization")

    # training details
    parser.add_argument("--weight_decay", default=0.0, type=float)
    parser.add_argument("--adam_epsilon", default=1e-8, type=float)
    parser.add_argument("--warmup_steps", default=0.0, type=float)

    args = parser.parse_args()

    # set up output dir which looks like './aste/rest14/extraction/'
    if not os.path.exists('./outputs'):
        os.mkdir('./outputs')

    task_dir = f"./outputs/{args.task}"
    if not os.path.exists(task_dir):
        os.mkdir(task_dir)

    task_dataset_dir = f"{task_dir}/{args.dataset}"
    if not os.path.exists(task_dataset_dir):
        os.mkdir(task_dataset_dir)

    output_dir = f"{task_dataset_dir}/{args.paradigm}"
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)
    args.output_dir = output_dir

    return args


def get_dataset(tokenizer, type_path, args):
    return ABSADataset(tokenizer=tokenizer, data_dir=args.dataset, data_type=type_path, 
                       paradigm=args.paradigm, task=args.task, max_len=args.max_seq_length)


class T5FineTuner(pl.LightningModule):
    def __init__(self, hparams):
        super(T5FineTuner, self).__init__()
        self.hparams = hparams

        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)
        self.tokenizer = T5Tokenizer.from_pretrained(hparams.model_name_or_path)

    def is_logger(self):
        return True

    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, 
                decoder_attention_mask=None, labels=None):
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=labels,
        )

    def _step(self, batch):
        lm_labels = batch["target_ids"]
        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100

        outputs = self(
            input_ids=batch["source_ids"],
            attention_mask=batch["source_mask"],
            labels=lm_labels,
            decoder_attention_mask=batch['target_mask']
        )

        loss = outputs[0]
        return loss

    def training_step(self, batch, batch_idx):
        loss = self._step(batch)

        tensorboard_logs = {"train_loss": loss}
        return {"loss": loss, "log": tensorboard_logs}

    def training_epoch_end(self, outputs):
        avg_train_loss = torch.stack([x["loss"] for x in outputs]).mean()
        tensorboard_logs = {"avg_train_loss": avg_train_loss}
        return {"avg_train_loss": avg_train_loss, "log": tensorboard_logs, 'progress_bar': tensorboard_logs}

    def validation_step(self, batch, batch_idx):
        loss = self._step(batch)
        return {"val_loss": loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        tensorboard_logs = {"val_loss": avg_loss}
        return {"avg_val_loss": avg_loss, "log": tensorboard_logs, 'progress_bar': tensorboard_logs}

    def configure_optimizers(self):
        '''Prepare optimizer and schedule (linear warmup and decay)'''
        model = self.model
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": self.hparams.weight_decay,
            },
            {
                "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)
        self.opt = optimizer
        return [optimizer]

    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):
        if self.trainer.use_tpu:
            xm.optimizer_step(optimizer)
        else:
            optimizer.step()
        optimizer.zero_grad()
        self.lr_scheduler.step()

    def get_tqdm_dict(self):
        tqdm_dict = {"loss": "{:.4f}".format(self.trainer.avg_loss), "lr": self.lr_scheduler.get_last_lr()[-1]}
        return tqdm_dict

    def train_dataloader(self):
        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path="train", args=self.hparams)
        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)
        t_total = (
            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, len(self.hparams.n_gpu))))
            // self.hparams.gradient_accumulation_steps
            * float(self.hparams.num_train_epochs)
        )
        scheduler = get_linear_schedule_with_warmup(
            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total
        )
        self.lr_scheduler = scheduler
        return dataloader

    def val_dataloader(self):
        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path="dev", args=self.hparams)
        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)


class LoggingCallback(pl.Callback):
    def on_validation_end(self, trainer, pl_module):
        logger.info("***** Validation results *****")
        if pl_module.is_logger():
            metrics = trainer.callback_metrics
        # Log results
        for key in sorted(metrics):
            if key not in ["log", "progress_bar"]:
                logger.info("{} = {}\n".format(key, str(metrics[key])))

    def on_test_end(self, trainer, pl_module):
        logger.info("***** Test results *****")

        if pl_module.is_logger():
            metrics = trainer.callback_metrics

        # Log and save results to file
        output_test_results_file = os.path.join(pl_module.hparams.output_dir, "test_results.txt")
        with open(output_test_results_file, "w") as writer:
            for key in sorted(metrics):
                if key not in ["log", "progress_bar"]:
                    logger.info("{} = {}\n".format(key, str(metrics[key])))
                    writer.write("{} = {}\n".format(key, str(metrics[key])))


def evaluate(data_loader, model, paradigm, task, sents):
    """
    Compute scores given the predictions and gold labels
    """
    device = torch.device(f'cuda:{args.n_gpu}')
    model.model.to(device)
    
    model.model.eval()
    outputs, targets = [], []
    for batch in tqdm(data_loader):
        # need to push the data to device
        print(batch['source_ids'])
        print(batch["target_ids"])
        print(batch["source_mask"])
        print(batch["target_mask"])
        outs = model.model.generate(input_ids=batch['source_ids'].to(device), 
                                    attention_mask=batch['source_mask'].to(device), 
                                    max_length=512)

        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]
        target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch["target_ids"]]

        outputs.extend(dec)
        targets.extend(target)

    raw_scores, fixed_scores, all_labels, all_preds, all_preds_fixed = compute_scores(outputs, targets, sents, paradigm, task)
    results = {'raw_scores': raw_scores, 'fixed_scores': fixed_scores, 'labels': all_labels,
               'preds': all_preds, 'preds_fixed': all_preds_fixed}
    # pickle.dump(results, open(f"{args.output_dir}/results-{args.task}-{args.dataset}-{args.paradigm}.pickle", 'wb'))

    return raw_scores, fixed_scores

def predict(data,tokenizer,model):
    """do predict"""
    device = torch.device(f'cuda:{args.n_gpu}')
    model.model.to(device)
    model.model.eval()
    inputs = tokenizer(
              data, max_length=args.max_seq_length, pad_to_max_length=True, truncation=True,
              return_tensors="pt",
            )
    outs = model.model.generate(input_ids=inputs["input_ids"].to(device), 
                                    attention_mask=inputs["attention_mask"].to(device), 
                                    max_length=1024)
    print(outs[0])
    dec=tokenizer.decode(outs[0], skip_special_tokens=True)
    

    return dec

# initialization
args = init_args()
print("\n", "="*30, f"NEW EXP: {args.task.upper()} on {args.dataset}", "="*30, "\n")

seed_everything(args.seed)

# tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)

# # show one sample to check the sanity of the code and the expected output
# print(f"Here is an example (from dev set) under `{args.paradigm}` paradigm:")
# dataset = ABSADataset(tokenizer=tokenizer, data_dir=args.dataset, data_type='dev', 
#                       paradigm=args.paradigm, task=args.task, max_len=args.max_seq_length)
# data_sample = dataset[2]  # a random data sample
# print('Input :', tokenizer.decode(data_sample['source_ids'], skip_special_tokens=True))
# print('Output:', tokenizer.decode(data_sample['target_ids'], skip_special_tokens=True))


# training process
if args.do_train:
    print("\n****** Conduct Training ******")
    model = T5FineTuner(args)

    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        filepath=args.output_dir, prefix="ckt", monitor='val_loss', mode='min', save_top_k=3
    )

    # prepare for trainer
    train_params = dict(
        default_root_dir=args.output_dir,
        accumulate_grad_batches=args.gradient_accumulation_steps,
        gpus=args.n_gpu,
        gradient_clip_val=1.0,
        #amp_level='O1',
        max_epochs=args.num_train_epochs,
        checkpoint_callback=checkpoint_callback,
        callbacks=[LoggingCallback()],
    )

    trainer = pl.Trainer(**train_params)
    trainer.fit(model)

    # save the final model
    # model.model.save_pretrained(args.output_dir)

    print("Finish training and saving the model!")


if args.do_eval:

    print("\n****** Conduct Evaluating ******")

    # model = T5FineTuner(args)
    dev_results, test_results = {}, {}
    best_f1, best_checkpoint, best_epoch = -999999.0, None, None
    all_checkpoints, all_epochs = [], []

    # retrieve all the saved checkpoints for model selection
    saved_model_dir = args.output_dir
    for f in os.listdir(saved_model_dir):
        file_name = os.path.join(saved_model_dir, f)
        if 'cktepoch' in file_name:
            all_checkpoints.append(file_name)

    # conduct some selection (or not)
    print(f"We will perform validation on the following checkpoints: {all_checkpoints}")

    # load dev and test datasets
    dev_dataset = ABSADataset(tokenizer, data_dir=args.dataset, data_type='dev',
                    paradigm=args.paradigm, task=args.task, max_len=args.max_seq_length)
    dev_loader = DataLoader(dev_dataset, batch_size=32, num_workers=4)

    test_dataset = ABSADataset(tokenizer, data_dir=args.dataset, data_type='test', 
                    paradigm=args.paradigm, task=args.task, max_len=args.max_seq_length)
    test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)
    
    for checkpoint in all_checkpoints:
        epoch = checkpoint.split('=')[-1][:-5] if len(checkpoint) > 1 else ""
        # only perform evaluation at the specific epochs ("15-19")
        # eval_begin, eval_end = args.eval_begin_end.split('-')
        if 0 <= int(epoch) < 100:
            all_epochs.append(epoch)

            # reload the model and conduct inference
            print(f"\nLoad the trained model from {checkpoint}...")
            model_ckpt = torch.load(checkpoint)
            model = T5FineTuner(model_ckpt['hyper_parameters'])
            model.load_state_dict(model_ckpt['state_dict'])
            
            dev_result = evaluate(dev_loader, model, args.paradigm, args.task)
            if dev_result['f1'] > best_f1:
                best_f1 = dev_result['f1']
                best_checkpoint = checkpoint
                best_epoch = epoch

            # add the global step to the name of these metrics for recording
            # 'f1' --> 'f1_1000'
            dev_result = dict((k + '_{}'.format(epoch), v) for k, v in dev_result.items())
            dev_results.update(dev_result)

            test_result = evaluate(test_loader, model, args.paradigm, args.task)
            test_result = dict((k + '_{}'.format(epoch), v) for k, v in test_result.items())
            test_results.update(test_result)

    # print test results over last few steps
    print(f"\n\nThe best checkpoint is {best_checkpoint}")
    best_step_metric = f"f1_{best_epoch}"
    print(f"F1 scores on test set: {test_results[best_step_metric]:.4f}")

    print("\n* Results *:  Dev  /  Test  \n")
    metric_names = ['f1', 'precision', 'recall']
    for epoch in all_epochs:
        print(f"Epoch-{epoch}:")
        for name in metric_names:
            name_step = f'{name}_{epoch}'
            print(f"{name:<10}: {dev_results[name_step]:.4f} / {test_results[name_step]:.4f}", sep='  ')
        print()

    results_log_dir = './results_log'
    if not os.path.exists(results_log_dir):
        os.mkdir(results_log_dir)
    log_file_path = f"{results_log_dir}/{args.task}-{args.dataset}.txt"
    write_results_to_log(log_file_path, test_results[best_step_metric], args, dev_results, test_results, all_epochs)


# evaluation process
if args.do_direct_eval:
    print("\n****** Conduct Evaluating with the last state ******")

    # model = T5FineTuner(args)

    # print("Reload the model")
    # model.model.from_pretrained(args.output_dir)

    sents, _ = read_line_examples_from_file(f'data/{args.task}/{args.dataset}/test.txt')

    print()
    test_dataset = ABSADataset(tokenizer, data_dir=args.dataset, data_type='test', 
                    paradigm=args.paradigm, task=args.task, max_len=args.max_seq_length)
    test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)
    # print(test_loader.device)
    raw_scores, fixed_scores = evaluate(test_loader, model, args.paradigm, args.task, sents)
    # print(scores)

    # write to file
    log_file_path = f"results_log/{args.task}-{args.dataset}.txt"
    local_time = time.asctime(time.localtime(time.time()))
    exp_settings = f"{args.task} on {args.dataset} under {args.paradigm}; Train bs={args.train_batch_size}, num_epochs = {args.num_train_epochs}"
    exp_results = f"Raw F1 = {raw_scores['f1']:.4f}, Fixed F1 = {fixed_scores['f1']:.4f}"
    log_str = f'============================================================\n'
    log_str += f"{local_time}\n{exp_settings}\n{exp_results}\n\n"
    with open(log_file_path, "a+") as f:
        f.write(log_str)

# prediction process
if args.do_direct_predict:
    print("\n****** Conduct predicting with the last state ******")
    checkpoint='./outputs/tasd-cn/ctrip/extraction/cktepoch=15_v1.ckpt'
#     checkpoint='./outputs/tasd-cn/ctrip/annotation/cktepoch=7.ckpt'
    print(f"\nLoad the trained model from {checkpoint}...")
    device=torch.device('cuda:0')
    model_ckpt = torch.load(checkpoint,map_location=device)
    model = T5FineTuner(model_ckpt['hyper_parameters'])
    model.load_state_dict(model_ckpt['state_dict'])
    tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)
    
    sents=['æ—©é¤ä¸€èˆ¬èˆ¬ï¼Œå‹‰å‹‰å¼ºå¼ºå¡«é¥±è‚šå­ï¼Œæ ·å¼å¯é€‰æ€§ä¸å¤šï¼Œå¯èƒ½æ˜¯ç–«æƒ…çš„å½±å“å§ã€‚ä¸è¿‡é…’åº—çš„æœåŠ¡ä¸é”™ï¼Œäº”ä¸ªå°å­©æ—©é¤éƒ½é€äº†ï¼Œç‚¹ðŸ‘ã€‚ç”±äºŽé…’åº—åŽ†å²æœ‰ç‚¹é•¿ï¼Œæ‰€ä»¥è®¾æ–½æ„Ÿè§‰ä¸€èˆ¬èˆ¬ï¼Œæ•´ä½“è¿˜å¯ä»¥ï¼Œä¸‰é’»å§',
           'æ¥¼ä¸‹å°±æ˜¯ä¸€å®¶åƒé¸¡çš„é¥­åº—ï¼Œå¥½å¤šäººæŽ’é˜Ÿï¼Œé—¨å‰å°±æ˜¯å…¬äº¤ç«™ï¼Œå‘¨è¾¹å°±æ˜¯è€åŸŽåŒºåƒä¸œè¥¿çš„åœ°æ–¹å¾ˆå¤šï¼Œæˆ¿é—´è¿˜ç®—å¹²å‡€å®½æ•ž',
           'å­©å­è¶…çº§å¼€å¿ƒï¼Œé…’åº—å¾ˆè´´å¿ƒï¼Œè¿˜æœ‰å„¿ç«¥æ‹–éž‹ï¼Œå„¿ç«¥ç”¨å“ï¼Œå„¿ç«¥æ¸¸ä¹åŒºï¼Œé€äº†å°çŽ©å…·å­©å­å¾ˆå–œæ¬¢ã€‚å¥½è¯„å¥½è¯„ã€‚',
           'æˆ¿é—´ç½‘é€Ÿè¶…å¿«ï¼Œæ‰“æ¸¸æˆç½‘ç»œä¸€ç‚¹éƒ½ä¸å¡ï¼Œå’Œæœ‹å‹ä»¬ä¸€èµ·å¼€é»‘çœŸçš„å¾ˆæ£’ï¼Œæˆ¿é—´å«ç”Ÿæ‰“æ‰«çš„ä¹Ÿå¾ˆå¹²å‡€æ•´æ´ï¼Œå®¤å†…è®¾æ–½ä¹Ÿå¾ˆé½å…¨ï¼Œæ€§ä»·æ¯”ä¹Ÿæ˜¯å¾ˆé«˜çš„',
           'å¤§å ‚å±…ç„¶æœ‰äººæŠ½çƒŸ.å·¥ä½œäººå‘˜ä¸åˆ¶æ­¢.ç”µæ¢¯çªç„¶å…³é—¨å·®ç‚¹å¤¹åˆ°å­©å­.æˆ¿é—´è®¾æ–½ç®€é™‹.å«ç”Ÿé—´åœ°å·¾æ˜¯ç ´çš„.çª—å¸˜å¾ˆè„.ç”µè§†åªæœ‰åä¸ªå°.å«ç”Ÿé—´é©¬æ¡¶ç›–åçš„.æ´—æ¾¡æ°´æ—¶å†·æ—¶çƒ­.æ€»ä¹‹å¾ˆå·®å¾ˆå·®çš„ä½“éªŒ.ä¸‹æ¬¡è‚¯å®šä¸ä¼šå†ä½',
           'æ¥ä¸Šæµ·è¿ªæ–¯å°¼æŽ¨èè¿™å®¶é…’åº—ï¼Œå¾ˆèˆ’æœï¼Œç»†èŠ‚åšå¾—å¾ˆå¥½ï¼Œå› ä¸ºæ˜¯äº²å­æˆ¿ï¼Œæ´—æ¼±å°ä¸‹é¢æœ‰ä¸ªå°å‡³å­ï¼Œæœ‰é€å„¿ç«¥ç‰™åˆ·ï¼Œæ˜¨å¤©çŸ¥é“é‡Œé¢ä½çš„ä¸¤ä¸ªå°æœ‹å‹ï¼Œæ•´ç†æˆ¿é—´çš„æ—¶å€™å°±é€äº†ä¸¤ä¸ªå„¿ç«¥ç‰™åˆ·ï¼Œæ‹–éž‹ä¹Ÿæœ‰å„¿ç«¥å°ºå¯¸çš„ï¼Œæœ‰æµ¦ä¸œæœºåœºæŽ¥é€æœºæœåŠ¡ï¼Œç¦»è¿ªæ–¯å°¼å¾ˆè¿‘ï¼Œ10åˆ†é’Ÿä¸åˆ°çš„è·¯ç¨‹ï¼Œä¹Ÿæœ‰ç­è½¦ï¼Œå¦‚æžœçº¯çŽ©è¿ªæ–¯å°¼ä½è¿™é‡Œæœ€ä½³ï¼Œå½“ç„¶åœŸè±ªå¯ä»¥ä½è¿ªæ–¯å°¼æ™¯åŒºçš„é…’åº—å’¯ã€‚'
           'ä¸ºäº†å¸¦å­©å­åŽ»è¿ªå£«å°¼çŽ©æ‰è®¢çš„è¿™å®¶ï¼Œæ‰“è½¦çš„å¸æœºæ€Žä¹ˆä¹Ÿæ‰¾ä¸åˆ°ï¼Œä¸‰ä¸ªäººå¼€ç€å¯¼èˆªä½¿åŠ²ç»•åœˆçœŸæ˜¯ã€‚å¥½åœ¨æˆ¿é—´è¿˜æ˜¯å¾ˆä¸é”™çš„ï¼Œæœ‰ç§‹åƒç»™å­©å­çŽ©ï¼Œè¿˜æœ‰æµ´ç¼¸å¯ä»¥æ³¡æ¾¡ï¼Œæ€»ä½“è¿˜æ˜¯ä¸é”™çš„ã€‚å®å®å¯¹å®¤å¤–çš„æ»‘æ¢¯ç‰¹åˆ«æ„Ÿå…´è¶£ï¼Œç®¡å®¶ä¹Ÿå¾ˆä¸é”™ï¼Œå¸¦å®å®è¿˜ç»™å®å®å¦å¤–å‡†å¤‡äº†æ—©é¤ï¼Œæ€»ä½“æ»¡æ„',
           'å®¢æ ˆåˆ°ç å¤´éžå¸¸è¿‘ã€‚ç å¤´ä¸‹èˆ¹ä»¥åŽå‡ºé—¨å·¦è¾¹ï¼Œè½¬ä¸€ä¸ªæ¹¾å°±åˆ°ï¼Œåˆ°æ™®æµŽå¯ºä¹Ÿå¾ˆæ–¹ä¾¿æ­¥è¡Œå‡ ç™¾ç±³ï¼Œè£…ä¿®æ¯”è¾ƒæœ‰é£Žæ ¼ï¼Œä½çš„äºŒæ¥¼æˆ¿é—´ä¸é”™ï¼Œè€æ¿å¾ˆçƒ­æƒ…ï¼Œæ—è¾¹åƒé¥­ä¹Ÿå¾ˆæ–¹ä¾¿ï¼Œå‡ºé—¨å°±æ˜¯å¥½å‡ ä¸ªé¥­åº—ï¼Œè¿˜æœ‰å°å–éƒ¨ã€‚å®¢æ ˆå«ç”Ÿä¹Ÿä¸é”™ï¼Œæˆ¿é—´å®‰é™ï¼Œä¸‹æ¬¡è¿‡åŽ»è¿˜ä½è¿™é‡Œã€‚è¿™æ¬¡æ˜¯æˆ‘ä¸€ä¸ªäººåŽ»çš„ï¼Œå®šäº†ä¸€ä¸ªåŒäººæ ‡é—´ã€‚æˆ¿é—´æ¯”è¾ƒç´§å‡‘ï¼Œä½†æ˜¯åˆå¾ˆæœ‰é£Žæ ¼ã€‚å¦‚æžœæœ‰æ—¶é—´çœŸçš„å¯ä»¥åœ¨è¿™é‡Œå°ä½ä¸€æ®µæ—¶é—´ã€‚å–œæ¬¢åšæ°‘å®¿çš„æœ‹å‹ï¼Œå¼ºçƒˆç»™å¤§å®¶æŽ¨èæ™®é™€å±±æ™®é™€å°é™¢å®¢æ ˆã€‚å¯¹äº†ï¼Œé¡ºä¾¿ç»™å¤§å®¶è¯´ä¸€ä¸‹ã€‚çº¿è·¯ä¸€ï¼Œå®¢æ ˆå‡ºé—¨ï¼Œåˆ°é©¬è·¯è¾¹ä¸Šï¼Œå¾€å³èµ°ï¼Œå°±æ˜¯åŽ»å—æµ·è§‚éŸ³ã€‚ä¹Ÿå¯ä»¥åˆ°ç å¤´åå¤§å·´è½¦ï¼Œäº”å—é’±åˆ°å—æµ·è§‚éŸ³åœè½¦åœºã€‚ç„¶åŽå¯ä»¥ä»Žå—æµ·è§‚éŸ³å‡ºæ¥ä»¥åŽï¼Œåˆ°ç´«ç«¹æž—å’Œä¸è‚¯åŽ»è§‚éŸ³å¯ºã€‚ç´«ç«¹æž—å‡ºæ¥ä»¥åŽä¹Ÿå¯ä»¥é€‰æ‹©æ­¥è¡Œåˆ°æ™®æµŽå¯ºï¼Œä¹Ÿå¯ä»¥åšå¤§å·´è½¦ï¼Œäº”å—é’±åˆ°æ™®æµŽå¯ºã€‚çº¿è·¯äºŒï¼Œå¾€å·¦èµ°å°±æ˜¯åŽ»æ™®æµŽå¯ºï¼Œä¹Ÿå¯ä»¥åå¤§å·´è½¦ï¼Œåˆ°è¥¿å±±æ™¯åŒºï¼Œä¸‹è½¦å¾€å‰èµ°200ç±³å·¦å³ï¼Œå°±æ˜¯æ™®æµŽå¯ºã€‚æ™®æµŽå¯ºå‡ºæ¥ä»¥åŽå¯ä»¥åŽ»ç™¾å­å ‚ã€‚ç„¶åŽå¯ä»¥é€‰æ‹©æ­¥è¡Œæˆ–è€…åå¤§å·´è½¦åŽ»å—æµ·è§‚éŸ³ï¼Œé‚£è¿˜è§‚éŸ³å‡ºæ¥åŽ»ç´«ç«¹æž—å’Œä¸è‚¯åŽ»è§‚éŸ³å¯ºã€‚ç„¶åŽå¯ä»¥åœ¨åœè½¦åœºï¼Œåè½¦åŽ»æ³•é›¨å¯ºï¼Œæ³•é›¨å¯ºå‡ºæ¥å¯ä»¥é€‰æ‹©ï¼Œçˆ¬å±±åŽ»ï¼Œæ…§æµŽå¯ºã€‚æ…§æµŽå¯ºå¯ä»¥åç´¢é“ä¸‹å±±åˆ°åœè½¦åœºç„¶åŽåè½¦åŽ»ï¼Œå–„è´¢æ´žå’Œæ¢µéŸ³æ´žã€‚ç„¶åŽé€‰æ‹©åšè½¦å›žç å¤´åˆ°å®¢æ ˆã€‚å› ä¸ºå®¢æ ˆç¦»ç å¤´å¾ˆè¿‘ï¼Œæ‰€ä»¥åˆ°ä»€ä¹ˆåœ°æ–¹åè½¦éƒ½å¾ˆæ–¹ä¾¿ã€‚ä»¥ä¸Šä»…ä¾›å¤§å®¶å‚è€ƒ'
          ]
    lab=[
        [('äº”ä¸ªå°å­©æ—©é¤éƒ½é€äº†', 'å„¿ç«¥é¤é¥®', 'äº”ä¸ªå°å­©æ—©é¤éƒ½é€äº†', 'å…¶ä»–', (43, 52), (43, 52))],
        [],
        [('å„¿ç«¥æ¸¸ä¹åŒº', 'å„¿ç«¥å¨±ä¹åŒº', 'æœ‰', 'å…¶ä»–', (25, 30), (14, 15))],
        [('å¼€é»‘', 'ç¬¼ç»Ÿæ¸¸æˆä½“éªŒ', 'çœŸçš„å¾ˆæ£’', 'æ­£', (24, 26), (26, 30))],
        [],
        
        [('æ»‘æ¢¯', 'å„¿ç«¥çŽ©å…·', 'æ»‘æ¢¯', 'å…¶ä»–', (84, 86), (84, 86))],
        
    ]
    s=time.time()
    for i in sents:
    # # print(test_loader.device)
        pred = predict(i, tokenizer,model)
        print('sents:',i)
        print('pred:',pred)
    e=time.time()
    print(e-s)

