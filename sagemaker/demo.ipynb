{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2c876c",
   "metadata": {},
   "source": [
    "# Training MMAction3 Mask-RCNN Model on Sagemaker Distributed Cluster\n",
    "\n",
    "## Motivation\n",
    "[MMDetection](https://github.com/open-mmlab/mmdetection) is a popular open-source Deep Learning framework focused on Computer Vision models and use cases. MMDetection provides to higher level APIs for model training and inference. It demonstrates [state-of-the-art benchmarks](https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo) for variety of model architecture and extensive Model Zoo.\n",
    "\n",
    "In this notebook, we will build a custom training container with MMdetection library and then train Mask-RCNN model from scratch on [COCO2017 dataset](https://cocodataset.org/#home) using Sagemaker distributed [training feature](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) in order to reduce training time.\n",
    "\n",
    "### Preconditions\n",
    "- To execute this notebook, you will need to have COCO 2017 training and validation datasets uploaded to S3 bucket available for Amazon Sagemaker service.\n",
    "\n",
    "\n",
    "## Building Training Container\n",
    "\n",
    "Amazon Sagemaker allows to BYO containers for training, data processing, and inference. In our case, we need to build custom training container which will be pushed to your AWS account [ECR service](https://aws.amazon.com/ecr/). \n",
    "\n",
    "For this, we need to login to public ECR with Sagemaker base images and private ECR reposity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c27cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc9f7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ee05e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m REGISTRY_URI\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m${REGISTRY_URI}/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m mkdir -p /opt/ml/model\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=TRUE\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONDONTWRITEBYTECODE\u001b[39;49;00m=TRUE\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/program:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mPATH\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m G_ABSA /opt/ml/code/Generative-ABSA-SageMaker\n",
      "\n",
      "\u001b[37m##########################################################################################\u001b[39;49;00m\n",
      "\u001b[37m# SageMaker requirements\u001b[39;49;00m\n",
      "\u001b[37m##########################################################################################\u001b[39;49;00m\n",
      "\u001b[37m## install flask\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[31mnetworkx\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.3 flask gevent gunicorn boto3 \u001b[31mtransformers\u001b[39;49;00m==\u001b[34m4\u001b[39;49;00m.6.0 \u001b[31mdatasets\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.11.0 \u001b[31msentencepiece\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.1.91 \u001b[31mpytorch_lightning\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.8.1 jieba editdistance -i https://opentuna.cn/pypi/web/simple\n",
      "\u001b[37m### Install nginx notebook\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get -y update && apt-get install -y --no-install-recommends \u001b[33m\\\u001b[39;49;00m\n",
      "         wget \u001b[33m\\\u001b[39;49;00m\n",
      "         nginx \u001b[33m\\\u001b[39;49;00m\n",
      "         ca-certificates \u001b[33m\\\u001b[39;49;00m\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "\u001b[37m# forward request and error logs to docker log collector\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m ln -sf /dev/stdout /var/log/nginx/access.log\n",
      "\u001b[34mRUN\u001b[39;49;00m ln -sf /dev/stderr /var/log/nginx/error.log\n",
      "\n",
      "\u001b[37m# Set up the program in the image\u001b[39;49;00m\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone https://github.com/HaoranLv/Generative-ABSA-SageMaker.git\n",
      "\u001b[34mCOPY\u001b[39;49;00m requirements.txt /opt/ml/code\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install -r requirements.txt\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM G_ABSA_train.py\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\u001b[37m# COPY * /opt/program/\u001b[39;49;00m\n",
      "\u001b[37m# COPY model/* /opt/program/model/\u001b[39;49;00m\n",
      "\u001b[37m# WORKDIR /opt/program\u001b[39;49;00m\n",
      "\u001b[37m# ENTRYPOINT [\"python\", \"predictor.py\"]\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker Dockerfile.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e1bf9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
      "# by SageMaker.\n",
      "\n",
      "# The argument to this script is the image name. This will be used as the image on the local\n",
      "# machine and combined with the account and region to form the repository name for ECR.\n",
      "image=$1\n",
      "\n",
      "if [ \"$image\" == \"\" ]\n",
      "then\n",
      "    echo \"Use image name absa\"\n",
      "    image=\"absa\"\n",
      "fi\n",
      "\n",
      "# Get the account number associated with the current IAM credentials\n",
      "account=$(aws sts get-caller-identity --query Account --output text)\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    exit 255\n",
      "fi\n",
      "\n",
      "# Get the region defined in the current configuration\n",
      "region=$(aws configure get region)\n",
      "#regions=$(aws ec2 describe-regions --all-regions --query \"Regions[].{Name:RegionName}\" --output text)\n",
      "\n",
      "#for region in $regions; do\n",
      "\n",
      "#aws s3 cp s3://aws-solutions-${region}/spot-bot-models/cars/model.tar.gz ./\n",
      "#tar zxvf model.tar.gz\n",
      "# TODO: update regional location based on https://amazonaws-china.com/releasenotes/available-deep-learning-containers-images/\n",
      "\n",
      "if [[ $region =~ ^cn.* ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com.cn/${image}:latest\"\n",
      "    registry_id=\"727897471807\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com.cn\"\n",
      "elif [[ $region = \"ap-east-1\" ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"871362719292\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "else\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"763104351884\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "fi\n",
      "\n",
      "echo ${fullname}\n",
      "847380964353.dkr.ecr.us-west-2.amazonaws.com/gabsa-training:latest\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "aws ecr describe-repositories --repository-names \"${image}\" --region ${region} || aws ecr create-repository --repository-name \"${image}\" --region ${region}\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-west-2:847380964353:repository/gabsa-training\",\n",
      "            \"registryId\": \"847380964353\",\n",
      "            \"repositoryName\": \"gabsa-training\",\n",
      "            \"repositoryUri\": \"847380964353.dkr.ecr.us-west-2.amazonaws.com/gabsa-training\",\n",
      "            \"createdAt\": 1641369116.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "$(aws ecr get-login --registry-ids ${account} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "$(aws ecr get-login --registry-ids ${registry_id} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "# Build the docker image, tag with full name and then push it to ECR\n",
      "docker build -t ${image} -f Dockerfile.training . --build-arg REGISTRY_URI=${registry_uri}\n",
      "Sending build context to Docker daemon  572.9kB\n",
      "Step 1/19 : ARG REGISTRY_URI\n",
      "Step 2/19 : FROM ${REGISTRY_URI}/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\n",
      "1.5.0-gpu-py36-cu101-ubuntu16.04: Pulling from pytorch-training\n",
      "\n",
      "\u001b[1Ba89234b4: Already exists \n",
      "\u001b[1B26c6b9c9: Already exists \n",
      "\u001b[1Bbf18aa40: Already exists \n",
      "\u001b[1Bc688ebe3: Already exists \n",
      "\u001b[1Bd5861307: Already exists \n",
      "\u001b[1B27b8f0ff: Already exists \n",
      "\u001b[1B81630d15: Already exists \n",
      "\u001b[1Be18332c4: Already exists \n",
      "\u001b[1Bdfb2533b: Already exists \n",
      "\u001b[1B60a54609: Already exists \n",
      "\u001b[1Bc09e1537: Already exists \n",
      "\u001b[1B7b98fd72: Already exists \n",
      "\u001b[1B78135d26: Already exists \n",
      "\u001b[1B33645603: Already exists \n",
      "\u001b[1Be13294f0: Already exists \n",
      "\u001b[1B6bb6b4f1: Already exists \n",
      "\u001b[1Bd648c9a6: Already exists \n",
      "\u001b[1B42b4bb9a: Already exists \n",
      "\u001b[1Bf820b79e: Already exists \n",
      "\u001b[1B3c32bbdf: Already exists \n",
      "\u001b[1Bf2dab4cd: Already exists \n",
      "\u001b[1Bb71aac7f: Already exists \n",
      "\u001b[1B92b1b177: Already exists \n",
      "\u001b[1B01f21cc2: Already exists \n",
      "\u001b[1Be8d53349: Already exists \n",
      "\u001b[1B6cfbec20: Already exists \n",
      "\u001b[1B7c947bf1: Already exists \n",
      "\u001b[1B564e61e1: Already exists \n",
      "\u001b[1B841e0809: Already exists \n",
      "\u001b[1B194652b6: Already exists \n",
      "\u001b[1B3e89995b: Already exists \n",
      "\u001b[1B562d733c: Already exists Digest: sha256:763ffe8b56c51c17844c881df99341c59434be71cdcadd7fde7c6669268469ca\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> e02a9fe9dd9a\n",
      "Step 3/19 : RUN mkdir -p /opt/ml/model\n",
      " ---> Running in f1a36f804616\n",
      "Removing intermediate container f1a36f804616\n",
      " ---> c8feed56ad49\n",
      "Step 4/19 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in 17c56b06a23b\n",
      "Removing intermediate container 17c56b06a23b\n",
      " ---> cac967988473\n",
      "Step 5/19 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in 29b811de009d\n",
      "Removing intermediate container 29b811de009d\n",
      " ---> 086dd9e23bf5\n",
      "Step 6/19 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Running in 5a29941bc9c4\n",
      "Removing intermediate container 5a29941bc9c4\n",
      " ---> 8bd373283db8\n",
      "Step 7/19 : ENV G_ABSA /opt/ml/code/Generative-ABSA-SageMaker\n",
      " ---> Running in 4dfbdf72ad09\n",
      "Removing intermediate container 4dfbdf72ad09\n",
      " ---> 5d6dfaefe142\n",
      "Step 8/19 : RUN pip install networkx==2.3 flask gevent gunicorn boto3 transformers==4.6.0 datasets==1.11.0 sentencepiece==0.1.91 pytorch_lightning==0.8.1 jieba editdistance -i https://opentuna.cn/pypi/web/simple\n",
      " ---> Running in 7e7341f04dab\n",
      "Looking in indexes: https://opentuna.cn/pypi/web/simple\n",
      "Collecting networkx==2.3\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7 MB)\n",
      "Collecting flask\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/8f/b6/b4fdcb6d01ee20f9cfe81dcf9d3cd6c2f874b996f186f1c0b898c4a59c04/Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: gevent in /opt/conda/lib/python3.6/site-packages (21.1.2)\n",
      "Collecting gunicorn\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/e4/dd/5b190393e6066286773a67dfcc2f9492058e9b57c4867a95f1ba5caf0a83/gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (1.17.26)\n",
      "Collecting transformers==4.6.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
      "Collecting datasets==1.11.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/9f/40/5d3df41be03e6947d7faaafe27bd9e4b94d3730b94e3189b679f11f9f390/datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pytorch_lightning==0.8.1\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/68/8e/d1bb6f3696aaed40bf8263c0d9be95593dbc6a63921cca68f0e7f60e7893/pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\n",
      "Collecting jieba\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "Collecting editdistance\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/b7/b0/d694a1b7b2464af12d66a4bf058ec393eed3bb5cafb1057cc999ff7c0e85/editdistance-0.6.0-cp36-cp36m-manylinux2010_x86_64.whl (284 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/df/d0/c3011a5bb77f307e68682a5046cce1a2c6591267bf24b5bf3fc4130bb39d/multiprocess-0.70.12.2-py36-none-any.whl (106 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (3.7.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (2.22.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (20.9)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/96/9a/0686eeeea343df547cbacde15c1bd958eb7a3f5c58291b44a0e2aef1c30c/huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "Collecting dill\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (0.25.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.42 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0) (4.56.0)\n",
      "Collecting xxhash\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/3f/b6/7162f78a301cedcf1b99e151aa16d774add3b81d086cf2b67a79871fbfec/xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\n",
      "Collecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/72/b5/01d4730395cf27ec679debfeab60efaafebd10177ff5b04eef5d530a6bf8/pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "Collecting fsspec>=2021.05.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/97/76/8f9921e10d9cbdf4790d4fb7a9883f3d636926e8b0564a4983ac15c167c9/fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx==2.3) (4.4.2)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1) (0.17.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1) (1.5.0)\n",
      "Collecting tensorboard>=1.14\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/2d/eb/80f75ab480cfbd032442f06ec7c15ef88376c5ef7fd6f6bf2e0e03b47e31/tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/ec/e5/407e634cbd3b96a9ce6960874c5b66829592ead9ac762bd50662244ce20b/sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting filelock\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/c2/99/dad689cc27a041a01376957c4c3b0147bcc537c93dc01e03e89ebc5df807/regex-2021.11.10-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/bf/20/3605db440db4f96d5ffd66b231a043ae451ec7e5e4d1a2fb6f20608006c4/tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.11.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.11.0) (1.25.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.11.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.11.0) (3.0.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/39/99/3ec65ac96cdac250f4aeb2c052c98826755801890288cd4198dbdd6926dc/grpcio-1.43.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/2c/03/e3e19d3faf430ede32e41221b294e37952e06acc96781c417ac25d4a0324/absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/6a/f7/06cbd5ebfba40a9d51df6217f9325317d824234400454aebcf014e2eee38/google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/9f/d4/2c7f83915d437736996b2674300c6c4b578a6f897f34e40f5c04db146719/Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1) (49.6.0.post20210108)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1) (3.15.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.8.1) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1) (4.7.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/ea/c1/4740af52db75e6dbdd57fc7e9478439815bbac549c1c05881be27d19a17d/cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets==1.11.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets==1.11.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/e8/5d/9dd1c29e5a786525f6342f6c1d812ed2e37edc653ad297048c1668988053/oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.26 in /opt/conda/lib/python3.6/site-packages (from boto3) (1.20.26)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.26->boto3) (2.8.1)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/9c/96/26f935afba9cd6140216da5add223a0c465b99d0f112b68a4ca426441019/itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/1e/73/51137805d1b8d97367a8a77cae4a792af14bb7ce58fbd071af294c740cf0/Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.6/site-packages (from flask) (7.1.2)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/20/9a/e5d9ec41927401e41aea8af6d16e78b5e612bca4699d417f646a9610a076/Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://opentuna.cn/pypi/web/packages/e2/a9/eafee9babd4b3aed918d286fbe1c20d1a22d347b30d2bddb3c49919548fa/MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: zope.event in /opt/conda/lib/python3.6/site-packages (from gevent) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.6/site-packages (from gevent) (5.2.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=0.4.17 in /opt/conda/lib/python3.6/site-packages (from gevent) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets==1.11.0) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0) (2021.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0) (1.0.1)\n",
      "Building wheels for collected packages: networkx, jieba\n",
      "  Building wheel for networkx (setup.py): started\n",
      "  Building wheel for networkx (setup.py): finished with status 'done'\n",
      "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1555990 sha256=b6a10f0fc0ba8a5007c57d24b17f6a5c20c35cc231064f1837bca1af3bc22c96\n",
      "  Stored in directory: /root/.cache/pip/wheels/3c/7c/83/1245693d0c795cc9b0b3d340e3a9db85ac829f7f349889fbe6\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=be36e0ccb37c754a7b53c02d149524075d61fcb0d115ad628421516361221252\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/a8/b4/e5b92284dfac9d4c68a1d314915c2a59f39d46441776934bcc\n",
      "Successfully built networkx jieba\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, regex, numpy, MarkupSafe, markdown, grpcio, google-auth-oauthlib, filelock, dill, absl-py, xxhash, tokenizers, tensorboard, sacremoses, pyarrow, multiprocess, Jinja2, itsdangerous, huggingface-hub, fsspec, transformers, sentencepiece, pytorch-lightning, networkx, jieba, gunicorn, flask, editdistance, datasets\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.2\n",
      "    Uninstalling importlib-metadata-3.7.2:\n",
      "      Successfully uninstalled importlib-metadata-3.7.2\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.5\n",
      "    Uninstalling networkx-2.5:\n",
      "      Successfully uninstalled networkx-2.5\n",
      "Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 absl-py-1.0.0 cachetools-4.2.4 datasets-1.11.0 dill-0.3.4 editdistance-0.6.0 filelock-3.4.1 flask-2.0.2 fsspec-2021.11.1 google-auth-2.3.3 google-auth-oauthlib-0.4.6 grpcio-1.43.0 gunicorn-20.1.0 huggingface-hub-0.0.8 importlib-metadata-4.8.3 itsdangerous-2.0.1 jieba-0.42.1 markdown-3.3.6 multiprocess-0.70.12.2 networkx-2.3 numpy-1.19.5 oauthlib-3.1.1 pyarrow-6.0.1 pyasn1-modules-0.2.8 pytorch-lightning-0.8.1 regex-2021.11.10 requests-oauthlib-1.3.0 sacremoses-0.0.47 sentencepiece-0.1.91 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.10.3 transformers-4.6.0 werkzeug-2.0.2 xxhash-2.0.2\n",
      "Removing intermediate container 7e7341f04dab\n",
      " ---> a7cdcc0e5d2c\n",
      "Step 9/19 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 44a1bc8a5809\n",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Get:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]\n",
      "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [696 B]\n",
      "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
      "Get:7 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [836 B]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [638 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [2051 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [2560 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [984 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [1544 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [12.7 kB]\n",
      "Fetched 8115 kB in 2s (3893 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "ca-certificates is already the newest version (20210119~16.04.1).\n",
      "wget is already the newest version (1.17.1-1ubuntu1.5).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-curand-dev-10-1 cuda-cusparse-dev-10-1\n",
      "Use 'apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  libgeoip1 libxslt1.1 nginx-common nginx-core\n",
      "Suggested packages:\n",
      "  geoip-bin fcgiwrap nginx-doc ssl-cert\n",
      "Recommended packages:\n",
      "  geoip-database\n",
      "The following NEW packages will be installed:\n",
      "  libgeoip1 libxslt1.1 nginx nginx-common nginx-core\n",
      "0 upgraded, 5 newly installed, 0 to remove and 34 not upgraded.\n",
      "Need to get 676 kB of archives.\n",
      "After this operation, 2191 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libgeoip1 amd64 1.6.9-1 [70.1 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libxslt1.1 amd64 1.1.28-2.1ubuntu0.3 [146 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx-common all 1.10.3-0ubuntu0.16.04.5 [26.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx-core amd64 1.10.3-0ubuntu0.16.04.5 [429 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 nginx all 1.10.3-0ubuntu0.16.04.5 [3494 B]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 676 kB in 1s (572 kB/s)\n",
      "Selecting previously unselected package libgeoip1:amd64.\n",
      "(Reading database ... 36487 files and directories currently installed.)\n",
      "Preparing to unpack .../libgeoip1_1.6.9-1_amd64.deb ...\n",
      "Unpacking libgeoip1:amd64 (1.6.9-1) ...\n",
      "Selecting previously unselected package libxslt1.1:amd64.\n",
      "Preparing to unpack .../libxslt1.1_1.1.28-2.1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libxslt1.1:amd64 (1.1.28-2.1ubuntu0.3) ...\n",
      "Selecting previously unselected package nginx-common.\n",
      "Preparing to unpack .../nginx-common_1.10.3-0ubuntu0.16.04.5_all.deb ...\n",
      "Unpacking nginx-common (1.10.3-0ubuntu0.16.04.5) ...\n",
      "Selecting previously unselected package nginx-core.\n",
      "Preparing to unpack .../nginx-core_1.10.3-0ubuntu0.16.04.5_amd64.deb ...\n",
      "Unpacking nginx-core (1.10.3-0ubuntu0.16.04.5) ...\n",
      "Selecting previously unselected package nginx.\n",
      "Preparing to unpack .../nginx_1.10.3-0ubuntu0.16.04.5_all.deb ...\n",
      "Unpacking nginx (1.10.3-0ubuntu0.16.04.5) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Processing triggers for systemd (229-4ubuntu21.29) ...\n",
      "Setting up libgeoip1:amd64 (1.6.9-1) ...\n",
      "Setting up libxslt1.1:amd64 (1.1.28-2.1ubuntu0.3) ...\n",
      "Setting up nginx-common (1.10.3-0ubuntu0.16.04.5) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up nginx-core (1.10.3-0ubuntu0.16.04.5) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up nginx (1.10.3-0ubuntu0.16.04.5) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu11.2) ...\n",
      "Processing triggers for systemd (229-4ubuntu21.29) ...\n",
      "Removing intermediate container 44a1bc8a5809\n",
      " ---> 5db0607b7f98\n",
      "Step 10/19 : RUN ln -sf /dev/stdout /var/log/nginx/access.log\n",
      " ---> Running in d4697ed42635\n",
      "Removing intermediate container d4697ed42635\n",
      " ---> 5c0e5aa247c6\n",
      "Step 11/19 : RUN ln -sf /dev/stderr /var/log/nginx/error.log\n",
      " ---> Running in 89a828bbcdca\n",
      "Removing intermediate container 89a828bbcdca\n",
      " ---> dae0de75f6bb\n",
      "Step 12/19 : WORKDIR /opt/ml/code\n",
      " ---> Running in 0ab35335d15a\n",
      "Removing intermediate container 0ab35335d15a\n",
      " ---> 90e036d9c9e0\n",
      "Step 13/19 : RUN git clone https://github.com/HaoranLv/Generative-ABSA-SageMaker.git\n",
      " ---> Running in 417ef3b73d40\n",
      "\u001b[91mCloning into 'Generative-ABSA-SageMaker'...\n",
      "\u001b[0mRemoving intermediate container 417ef3b73d40\n",
      " ---> fa4afaa44c7b\n",
      "Step 14/19 : COPY requirements.txt /opt/ml/code\n",
      " ---> c352908c001b\n",
      "Step 15/19 : RUN pip install -r requirements.txt\n",
      " ---> Running in 43890d7b5a53\n",
      "Collecting transformers==4.0.0\n",
      "  Downloading transformers-4.0.0-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\n",
      "Requirement already satisfied: pytorch_lightning==0.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.42.1)\n",
      "Requirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.19.5)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.56.0)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.7.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2.22.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.0.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (3.4.1)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.15.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.43.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.0.0->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.0\n",
      "    Uninstalling transformers-4.6.0:\n",
      "      Successfully uninstalled transformers-4.6.0\n",
      "Successfully installed tokenizers-0.9.4 transformers-4.0.0\n",
      "Removing intermediate container 43890d7b5a53\n",
      " ---> 609dd1c042b7\n",
      "Step 16/19 : COPY container_training /opt/ml/code\n",
      " ---> bac11f05dcda\n",
      "Step 17/19 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 3a4e5c7d2903\n",
      "Removing intermediate container 3a4e5c7d2903\n",
      " ---> c8a62c7d4de0\n",
      "Step 18/19 : ENV SAGEMAKER_PROGRAM G_ABSA_train.py\n",
      " ---> Running in ba1820e906fc\n",
      "Removing intermediate container ba1820e906fc\n",
      " ---> feb4b051ea62\n",
      "Step 19/19 : WORKDIR /\n",
      " ---> Running in 08c8fae6363e\n",
      "Removing intermediate container 08c8fae6363e\n",
      " ---> dea362382d92\n",
      "Successfully built dea362382d92\n",
      "Successfully tagged gabsa-training:latest\n",
      "docker tag ${image} ${fullname}\n",
      "docker push ${fullname}\n",
      "The push refers to repository [847380964353.dkr.ecr.us-west-2.amazonaws.com/gabsa-training]\n",
      "\n",
      "\u001b[1Bef35611f: Preparing \n",
      "\u001b[1B0e4c264b: Preparing \n",
      "\u001b[1B1f54c587: Preparing \n",
      "\u001b[1Bcde296cd: Preparing \n",
      "\u001b[1B52f32ea3: Preparing \n",
      "\u001b[1B39a16335: Preparing \n",
      "\u001b[1B87103684: Preparing \n",
      "\u001b[1B1842c10a: Preparing \n",
      "\u001b[1B37f9c62b: Preparing \n",
      "\u001b[1Bb2d1ee90: Preparing \n",
      "\u001b[1B4a822129: Preparing \n",
      "\u001b[1Bb0fd595e: Preparing \n",
      "\u001b[1B6f4ec134: Preparing \n",
      "\u001b[1Bd119a8b3: Preparing \n",
      "\u001b[1B03607d4a: Preparing \n",
      "\u001b[1B83dba423: Preparing \n",
      "\u001b[1B223e80e9: Preparing \n",
      "\u001b[1Bb771bd83: Preparing \n",
      "\u001b[1B89eadfc7: Preparing \n",
      "\u001b[1B1fa8596a: Preparing \n",
      "\u001b[1B7b248eeb: Preparing \n",
      "\u001b[16B7103684: Waiting g \n",
      "\u001b[14B2d1ee90: Waiting g \n",
      "\u001b[1B9d0c86d2: Preparing \n",
      "\u001b[18B842c10a: Waiting g \n",
      "\u001b[3B9d0c86d2: Waiting g \n",
      "\u001b[1B7ba36366: Preparing \n",
      "\u001b[1B0dfc111e: Preparing \n",
      "\u001b[1B35343568: Preparing \n",
      "\u001b[1B68f21cbb: Preparing \n",
      "\u001b[1Bff006560: Preparing \n",
      "\u001b[5B0dfc111e: Waiting g \n",
      "\u001b[1B2ea48d23: Preparing \n",
      "\u001b[8B7ba36366: Waiting g \n",
      "\u001b[1B6d30da44: Preparing \n",
      "\u001b[4B2ea48d23: Waiting g \n",
      "\u001b[2Bd96b0113: Waiting g \n",
      "\u001b[1B39edd0b8: Preparing \n",
      "\u001b[1Bd2b930fc: Preparing \n",
      "\u001b[1Bec0db89a: Preparing \n",
      "\u001b[1B49baa658: Preparing \n",
      "\u001b[34B7f9c62b: Pushed   408.9MB/403.1MB\u001b[42A\u001b[2K\u001b[39A\u001b[2K\u001b[41A\u001b[2K\u001b[38A\u001b[2K\u001b[41A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[35A\u001b[2K\u001b[41A\u001b[2K\u001b[35A\u001b[2K\u001b[41A\u001b[2K\u001b[35A\u001b[2K\u001b[41A\u001b[2K\u001b[34A\u001b[2K\u001b[41A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[30A\u001b[2K\u001b[28A\u001b[2K\u001b[34A\u001b[2K\u001b[24A\u001b[2K\u001b[21A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[39A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[4A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[1A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[39A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2Klatest: digest: sha256:7871477d2e4a8ea10a2c15a9c641be8eeddc8acdd3d998a2b54ae6a8354c1c65 size: 9128\n",
      "\n",
      "#done\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh gabsa-training latest Dockerfile.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20455758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72dbe845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "prefix_input = 'gabsa-input'\n",
    "prefix_output = 'gabsa-ouput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d407421",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"gabsa-training\" # your container name\n",
    "tag = \"latest\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cc4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"task\" : \"tasd\", \n",
    "    \"data_root\" : \"/opt/ml/input/data/training\",\n",
    "    \"dataset\" : \"rest16\",\n",
    "    \"n_gpu\":\"4\",\n",
    "    \"model_name_or_path\" : \"t5-base\", \n",
    "    \"paradigm\": \"extraction\",\n",
    "    \"gradient_accumulation_steps\": \"2\",\n",
    "    \"eval_batch_size\" :\"16\",\n",
    "    \"train_batch_size\" :\"2\",\n",
    "    \"learning_rate\" :\"3e-4\",\n",
    "    \"num_train_epochs\":\"2\",\n",
    "    \"out_dir\":\"/opt/ml/output\",\n",
    "    \"nodes\":\"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e8c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/aope/raw-data/16res/test.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/16res/test.tsv\n",
      "upload: ../data/aope/laptop14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aope/laptop14/dev.txt\n",
      "upload: ../data/aope/raw-data/15res/train.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/15res/train.tsv\n",
      "upload: ../data/aope/laptop14/test.txt to s3://sagemaker-us-west-2-847380964353/data/aope/laptop14/test.txt\n",
      "upload: ../data/aope/laptop14/train.txt to s3://sagemaker-us-west-2-847380964353/data/aope/laptop14/train.txt\n",
      "upload: ../data/aope/rest14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest14/dev.txt\n",
      "upload: ../data/aope/raw-data/14res/test.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/14res/test.tsv\n",
      "upload: ../data/aope/rest15/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest15/dev.txt\n",
      "upload: ../data/aope/raw-data/15res/test.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/15res/test.tsv\n",
      "upload: ../data/aope/raw-data/14lap/test.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/14lap/test.tsv\n",
      "upload: ../data/aope/raw-data/14lap/train.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/14lap/train.tsv\n",
      "upload: ../data/aope/raw-data/14res/train.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/14res/train.tsv\n",
      "upload: ../data/aope/rest15/test.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest15/test.txt\n",
      "upload: ../data/aope/raw-data/16res/train.tsv to s3://sagemaker-us-west-2-847380964353/data/aope/raw-data/16res/train.tsv\n",
      "upload: ../data/aope/rest14/train.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest14/train.txt\n",
      "upload: ../data/aope/rest14/test.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest14/test.txt\n",
      "upload: ../data/aope/rest16/test.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest16/test.txt\n",
      "upload: ../data/aope/rest16/train.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest16/train.txt\n",
      "upload: ../data/aope/rest15/train.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest15/train.txt\n",
      "upload: ../data/aope/rest16/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aope/rest16/dev.txt\n",
      "upload: ../data/aste/laptop14/test.txt to s3://sagemaker-us-west-2-847380964353/data/aste/laptop14/test.txt\n",
      "upload: ../data/aste/laptop14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aste/laptop14/dev.txt\n",
      "upload: ../data/aste/laptop14/train.txt to s3://sagemaker-us-west-2-847380964353/data/aste/laptop14/train.txt\n",
      "upload: ../data/aste/rest16/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest16/dev.txt\n",
      "upload: ../data/aste/rest14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest14/dev.txt\n",
      "upload: ../data/aste/rest14/train.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest14/train.txt\n",
      "upload: ../data/aste/rest15/dev.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest15/dev.txt\n",
      "upload: ../data/tasd/rest15/dev.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest15/dev.txt\n",
      "upload: ../data/aste/rest14/test.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest14/test.txt\n",
      "upload: ../data/aste/rest15/test.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest15/test.txt\n",
      "upload: ../data/aste/rest16/test.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest16/test.txt\n",
      "upload: ../data/aste/rest15/train.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest15/train.txt\n",
      "upload: ../data/aste/rest16/train.txt to s3://sagemaker-us-west-2-847380964353/data/aste/rest16/train.txt\n",
      "upload: ../data/tasd/rest16/dev.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest16/dev.txt\n",
      "upload: ../data/tasd/raw-data/ABSA_15_Restaurants_Test.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/raw-data/ABSA_15_Restaurants_Test.txt\n",
      "upload: ../data/tasd/rest15/test.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest15/test.txt\n",
      "upload: ../data/tasd/rest16/train.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest16/train.txt\n",
      "upload: ../data/tasd/rest16/test.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest16/test.txt\n",
      "upload: ../data/tasd/rest15/train.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/rest15/train.txt\n",
      "upload: ../data/tasd/raw-data/ABSA_15_Restaurants_Train.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/raw-data/ABSA_15_Restaurants_Train.txt\n",
      "upload: ../data/tasd/raw-data/ABSA_16_Restaurants_Test.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/raw-data/ABSA_16_Restaurants_Test.txt\n",
      "upload: ../data/uabsa/rest14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest14/dev.txt\n",
      "upload: ../data/uabsa/laptop14/dev.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/laptop14/dev.txt\n",
      "upload: ../data/tasd/raw-data/ABSA_16_Restaurants_Train.txt to s3://sagemaker-us-west-2-847380964353/data/tasd/raw-data/ABSA_16_Restaurants_Train.txt\n",
      "upload: ../data/uabsa/laptop14/test.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/laptop14/test.txt\n",
      "upload: ../data/uabsa/rest16/dev.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest16/dev.txt\n",
      "upload: ../data/uabsa/laptop14/train.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/laptop14/train.txt\n",
      "upload: ../data/uabsa/rest15/dev.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest15/dev.txt\n",
      "upload: ../data/uabsa/rest15/test.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest15/test.txt\n",
      "upload: ../data/uabsa/rest15/train.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest15/train.txt\n",
      "upload: ../data/uabsa/rest14/train.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest14/train.txt\n",
      "upload: ../data/uabsa/rest14/test.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest14/test.txt\n",
      "upload: ../data/uabsa/rest16/test.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest16/test.txt\n",
      "upload: ../data/uabsa/rest16/train.txt to s3://sagemaker-us-west-2-847380964353/data/uabsa/rest16/train.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive ../data s3://$bucket/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b82dbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 07:30:20 Starting - Starting the training job...\n",
      "2022-01-10 07:30:46 Starting - Launching requested ML instancesProfilerReport-1641799820: InProgress\n",
      ".........\n",
      "2022-01-10 07:32:15 Starting - Preparing the instances for training......\n",
      "2022-01-10 07:33:17 Downloading - Downloading input data\n",
      "2022-01-10 07:33:17 Training - Downloading the training image...................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,405 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,436 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:23,448 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,213 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,244 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,245 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,245 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,245 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,245 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,245 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:26,257 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:27,719 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:27,721 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.0.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_lightning==0.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.19.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.56.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2021.11.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.0.47)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.43.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.36.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.15.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.0.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:29,673 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:29,675 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers==4.0.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytorch_lightning==0.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.42.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.56.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.19.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.17.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.0.47)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2021.11.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.0.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.36.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.15.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (2.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.43.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.15.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (4.8.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 3)) (3.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.0.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,155 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,155 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,155 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,156 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,156 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,156 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,197 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,238 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,238 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,238 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,239 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,239 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,239 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2022-01-10 07:36:30,251 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data_root\": \"/opt/ml/input/data/training\",\n",
      "        \"n_gpu\": 4,\n",
      "        \"paradigm\": \"extraction\",\n",
      "        \"eval_batch_size\": 16,\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"train_batch_size\": 2,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"nodes\": 2,\n",
      "        \"task\": \"tasd\",\n",
      "        \"out_dir\": \"/opt/ml/output\",\n",
      "        \"dataset\": \"rest16\",\n",
      "        \"learning_rate\": 0.0003,\n",
      "        \"model_name_or_path\": \"t5-base\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"gabsa-training-2022-01-10-07-30-20-217\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"G_ABSA_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"G_ABSA_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data_root\":\"/opt/ml/input/data/training\",\"dataset\":\"rest16\",\"eval_batch_size\":16,\"gradient_accumulation_steps\":2,\"learning_rate\":0.0003,\"model_name_or_path\":\"t5-base\",\"n_gpu\":4,\"nodes\":2,\"num_train_epochs\":2,\"out_dir\":\"/opt/ml/output\",\"paradigm\":\"extraction\",\"task\":\"tasd\",\"train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=G_ABSA_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=G_ABSA_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"data_root\":\"/opt/ml/input/data/training\",\"dataset\":\"rest16\",\"eval_batch_size\":16,\"gradient_accumulation_steps\":2,\"learning_rate\":0.0003,\"model_name_or_path\":\"t5-base\",\"n_gpu\":4,\"nodes\":2,\"num_train_epochs\":2,\"out_dir\":\"/opt/ml/output\",\"paradigm\":\"extraction\",\"task\":\"tasd\",\"train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"gabsa-training-2022-01-10-07-30-20-217\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"G_ABSA_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"G_ABSA_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data_root\",\"/opt/ml/input/data/training\",\"--dataset\",\"rest16\",\"--eval_batch_size\",\"16\",\"--gradient_accumulation_steps\",\"2\",\"--learning_rate\",\"0.0003\",\"--model_name_or_path\",\"t5-base\",\"--n_gpu\",\"4\",\"--nodes\",\"2\",\"--num_train_epochs\",\"2\",\"--out_dir\",\"/opt/ml/output\",\"--paradigm\",\"extraction\",\"--task\",\"tasd\",\"--train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_ROOT=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_N_GPU=4\u001b[0m\n",
      "\u001b[34mSM_HP_PARADIGM=extraction\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_NODES=2\u001b[0m\n",
      "\u001b[34mSM_HP_TASK=tasd\u001b[0m\n",
      "\u001b[34mSM_HP_OUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET=rest16\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0003\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=t5-base\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 G_ABSA_train.py --data_root /opt/ml/input/data/training --dataset rest16 --eval_batch_size 16 --gradient_accumulation_steps 2 --learning_rate 0.0003 --model_name_or_path t5-base --n_gpu 4 --nodes 2 --num_train_epochs 2 --out_dir /opt/ml/output --paradigm extraction --task tasd --train_batch_size 2\u001b[0m\n",
      "\u001b[34mStarting training...\u001b[0m\n",
      "\u001b[34mFollowing command will be executed: \n",
      " MASTER_ADDR=algo-1 MASTER_PORT=55555 WORLD_SIZE=8 NODE_RANK=0 python /opt/ml/code/Generative-ABSA-SageMaker/main.py --dataset rest16 --data_root /opt/ml/input/data/training --task tasd --model_name_or_path t5-base --n_gpu 4 --do_train --train_batch_size 2 --gradient_accumulation_steps 2 --eval_batch_size 16 --learning_rate 0.0003 --num_train_epochs 2 --nodes 2 --out_dir /opt/ml/output\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,898 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,940 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,940 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,940 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,940 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,940 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,941 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter data_root value /opt/ml/input/data/training to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter paradigm value extraction to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter task value tasd to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter out_dir value /opt/ml/output to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter dataset value rest16 to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,982 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value t5-base to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2022-01-10 07:36:31,994 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data_root\": \"/opt/ml/input/data/training\",\n",
      "        \"n_gpu\": 4,\n",
      "        \"paradigm\": \"extraction\",\n",
      "        \"eval_batch_size\": 16,\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"train_batch_size\": 2,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"nodes\": 2,\n",
      "        \"task\": \"tasd\",\n",
      "        \"out_dir\": \"/opt/ml/output\",\n",
      "        \"dataset\": \"rest16\",\n",
      "        \"learning_rate\": 0.0003,\n",
      "        \"model_name_or_path\": \"t5-base\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"gabsa-training-2022-01-10-07-30-20-217\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"G_ABSA_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"G_ABSA_train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"data_root\":\"/opt/ml/input/data/training\",\"dataset\":\"rest16\",\"eval_batch_size\":16,\"gradient_accumulation_steps\":2,\"learning_rate\":0.0003,\"model_name_or_path\":\"t5-base\",\"n_gpu\":4,\"nodes\":2,\"num_train_epochs\":2,\"out_dir\":\"/opt/ml/output\",\"paradigm\":\"extraction\",\"task\":\"tasd\",\"train_batch_size\":2}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=G_ABSA_train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=G_ABSA_train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"data_root\":\"/opt/ml/input/data/training\",\"dataset\":\"rest16\",\"eval_batch_size\":16,\"gradient_accumulation_steps\":2,\"learning_rate\":0.0003,\"model_name_or_path\":\"t5-base\",\"n_gpu\":4,\"nodes\":2,\"num_train_epochs\":2,\"out_dir\":\"/opt/ml/output\",\"paradigm\":\"extraction\",\"task\":\"tasd\",\"train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"gabsa-training-2022-01-10-07-30-20-217\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"G_ABSA_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"G_ABSA_train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--data_root\",\"/opt/ml/input/data/training\",\"--dataset\",\"rest16\",\"--eval_batch_size\",\"16\",\"--gradient_accumulation_steps\",\"2\",\"--learning_rate\",\"0.0003\",\"--model_name_or_path\",\"t5-base\",\"--n_gpu\",\"4\",\"--nodes\",\"2\",\"--num_train_epochs\",\"2\",\"--out_dir\",\"/opt/ml/output\",\"--paradigm\",\"extraction\",\"--task\",\"tasd\",\"--train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_DATA_ROOT=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_N_GPU=4\u001b[0m\n",
      "\u001b[35mSM_HP_PARADIGM=extraction\u001b[0m\n",
      "\u001b[35mSM_HP_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[35mSM_HP_NODES=2\u001b[0m\n",
      "\u001b[35mSM_HP_TASK=tasd\u001b[0m\n",
      "\u001b[35mSM_HP_OUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET=rest16\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0003\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME_OR_PATH=t5-base\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 G_ABSA_train.py --data_root /opt/ml/input/data/training --dataset rest16 --eval_batch_size 16 --gradient_accumulation_steps 2 --learning_rate 0.0003 --model_name_or_path t5-base --n_gpu 4 --nodes 2 --num_train_epochs 2 --out_dir /opt/ml/output --paradigm extraction --task tasd --train_batch_size 2\u001b[0m\n",
      "\u001b[35mStarting training...\u001b[0m\n",
      "\u001b[35mFollowing command will be executed: \n",
      " MASTER_ADDR=algo-1 MASTER_PORT=55555 WORLD_SIZE=8 NODE_RANK=1 python /opt/ml/code/Generative-ABSA-SageMaker/main.py --dataset rest16 --data_root /opt/ml/input/data/training --task tasd --model_name_or_path t5-base --n_gpu 4 --do_train --train_batch_size 2 --gradient_accumulation_steps 2 --eval_batch_size 16 --learning_rate 0.0003 --num_train_epochs 2 --nodes 2 --out_dir /opt/ml/output\u001b[0m\n",
      "\u001b[34m============================== NEW EXP: TASD on rest16 ==============================\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/792k [00:00<?, ?B/s]#015Downloading:   4%|▍         | 30.7k/792k [00:00<00:03, 211kB/s]#015Downloading:  25%|██▌       | 199k/792k [00:00<00:00, 876kB/s] #015Downloading:  56%|█████▌    | 444k/792k [00:00<00:00, 1.47MB/s]#015Downloading: 100%|██████████| 792k/792k [00:00<00:00, 1.79MB/s]\u001b[0m\n",
      "\u001b[34mHere is an example (from dev set) under `extraction` paradigm:\u001b[0m\n",
      "\u001b[34mTotal examples = 29\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\u001b[0m\n",
      "\u001b[34mFutureWarning,\u001b[0m\n",
      "\u001b[34mInput : They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.\u001b[0m\n",
      "\u001b[34mOutput: (NULL, service general, negative)\u001b[0m\n",
      "\u001b[34m****** Conduct Training ******\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 1.42MB/s]\u001b[0m\n",
      "\n",
      "2022-01-10 07:36:47 Training - Training image download completed. Training in progress.\u001b[35m============================== NEW EXP: TASD on rest16 ==============================\u001b[0m\n",
      "\u001b[35mHere is an example (from dev set) under `extraction` paradigm:\u001b[0m\n",
      "\u001b[35mTotal examples = 29\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\u001b[0m\n",
      "\u001b[35mFutureWarning,\u001b[0m\n",
      "\u001b[35mInput : They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.\u001b[0m\n",
      "\u001b[35mOutput: (NULL, service general, negative)\u001b[0m\n",
      "\u001b[35m****** Conduct Training ******\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35minitializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34minitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=ddp\u001b[0m\n",
      "\u001b[34mAll DDP processes registered. Starting ddp with 8 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m| Name  | Type                       | Params\u001b[0m\n",
      "\u001b[34m-----------------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | T5ForConditionalGeneration | 222 M\u001b[0m\n",
      "\u001b[34mTotal examples = 29\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\u001b[0m\n",
      "\u001b[34mwarnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[35mMissing logger folder: /opt/ml/output/tasd/rest16/extraction/lightning_logs\u001b[0m\n",
      "\u001b[35mTotal examples = 29\u001b[0m\n",
      "\u001b[35mTotal examples = 1708\u001b[0m\n",
      "\u001b[34mValidation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  1.03it/s]#015                                                                      #015Total examples = 1708\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\u001b[0m\n",
      "\u001b[34mwarnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mTotal examples = 29\u001b[0m\n",
      "\u001b[35mTotal examples = 29\u001b[0m\n",
      "\u001b[35mINFO:__main__:***** Validation results *****\u001b[0m\n",
      "\u001b[35mINFO:__main__:avg_val_loss = tensor(0.3018, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:loss = tensor(0.2627, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:train_loss = tensor(0.2627, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:val_loss = tensor(0.3018, device='cuda:0')\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]#015Training:   0%|          | 0/108 [00:00<?, ?it/s]#015Epoch 1:   0%|          | 0/108 [00:00<?, ?it/s] #015Epoch 1:   1%|          | 1/108 [00:04<08:18,  4.65s/it]#015Epoch 1:   1%|          | 1/108 [00:04<08:18,  4.65s/it, loss=nan, v_num=0]#015Epoch 1:   2%|▏         | 2/108 [00:07<06:18,  3.57s/it, loss=nan, v_num=0]#015Epoch 1:   2%|▏         | 2/108 [00:07<06:18,  3.57s/it, loss=2.621, v_num=0]#015Epoch 1:   3%|▎         | 3/108 [00:08<04:58,  2.84s/it, loss=2.621, v_num=0]#015Epoch 1:   3%|▎         | 3/108 [00:08<04:58,  2.84s/it, loss=2.621, v_num=0]#015Epoch 1:   4%|▎         | 4/108 [00:09<04:13,  2.44s/it, loss=2.621, v_num=0]#015Epoch 1:   4%|▎         | 4/108 [00:09<04:13,  2.44s/it, loss=2.517, v_num=0]#015Epoch 1:   5%|▍         | 5/108 [00:11<03:52,  2.25s/it, loss=2.517, v_num=0]#015Epoch 1:   5%|▍         | 5/108 [00:11<03:52,  2.25s/it, loss=2.517, v_num=0]#015Epoch 1:   6%|▌         | 6/108 [00:12<03:35,  2.11s/it, loss=2.517, v_num=0]#015Epoch 1:   6%|▌         | 6/108 [00:12<03:35,  2.11s/it, loss=2.223, v_num=0]#015Epoch 1:   6%|▋         | 7/108 [00:14<03:27,  2.06s/it, loss=2.223, v_num=0]#015Epoch 1:   6%|▋         | 7/108 [00:14<03:27,  2.06s/it, loss=2.223, v_num=0]#015Epoch 1:   7%|▋         | 8/108 [00:16<03:21,  2.01s/it, loss=2.223, v_num=0]#015Epoch 1:   7%|▋         | 8/108 [00:16<03:21,  2.01s/it, loss=2.115, v_num=0]#015Epoch 1:   8%|▊         | 9/108 [00:17<03:16,  1.99s/it, loss=2.115, v_num=0]#015Epoch 1:   8%|▊         | 9/108 [00:17<03:16,  1.99s/it, loss=2.115, v_num=0]#015Epoch 1:   9%|▉         | 10/108 [00:20<03:16,  2.00s/it, loss=2.115, v_num=0]#015Epoch 1:   9%|▉         | 10/108 [00:20<03:16,  2.00s/it, loss=1.966, v_num=0]#015Epoch 1:  10%|█         | 11/108 [00:22<03:15,  2.02s/it, loss=1.966, v_num=0]#015Epoch 1:  10%|█         | 11/108 [00:22<03:15,  2.02s/it, loss=1.966, v_num=0]#015Epoch 1:  11%|█         | 12/108 [00:23<03:07,  1.96s/it, loss=1.966, v_num=0]#015Epoch 1:  11%|█         | 12/108 [00:23<03:07,  1.96s/it, loss=1.820, v_num=0]#015Epoch 1:  12%|█▏        | 13/108 [00:24<03:02,  1.92s/it, loss=1.820, v_num=0]#015Epoch 1:  12%|█▏        | 13/108 [00:24<03:02,  1.92s/it, loss=1.820, v_num=0]#015Epoch 1:  13%|█▎        | 14/108 [00:26<03:00,  1.92s/it, loss=1.820, v_num=0]#015Epoch 1:  13%|█▎        | 14/108 [00:26<03:00,  1.92s/it, loss=1.700, v_num=0]#015Epoch 1:  14%|█▍        | 15/108 [00:28<02:54,  1.87s/it, loss=1.700, v_num=0]#015Epoch 1:  14%|█▍        | 15/108 [00:28<02:54,  1.87s/it, loss=1.700, v_num=0]#015Epoch 1:  15%|█▍        | 16/108 [00:29<02:49,  1.84s/it, loss=1.700, v_num=0]#015Epoch 1:  15%|█▍        | 16/108 [00:29<02:49,  1.84s/it, loss=1.602, v_num=0]#015Epoch 1:  16%|█▌        | 17/108 [00:31<02:47,  1.84s/it, loss=1.602, v_num=0]#015Epoch 1:  16%|█▌        | 17/108 [00:31<02:47,  1.84s/it, loss=1.602, v_num=0]#015Epoch 1:  17%|█▋        | 18/108 [00:32<02:43,  1.82s/it, loss=1.602, v_num=0]#015Epoch 1:  17%|█▋        | 18/108 [00:32<02:43,  1.82s/it, loss=1.522, v_num=0]#015Epoch 1:  18%|█▊        | 19/108 [00:33<02:38,  1.78s/it, loss=1.522, v_num=0]#015Epoch 1:  18%|█▊        | 19/108 [00:33<02:38,  1.78s/it, loss=1.522, v_num=0]#015Epoch 1:  19%|█▊        | 20/108 [00:35<02:36,  1.78s/it, loss=1.522, v_num=0]#015Epoch 1:  19%|█▊        | 20/108 [00:35<02:36,  1.78s/it, loss=1.446, v_num=0]#015Epoch 1:  19%|█▉        | 21/108 [00:37<02:36,  1.80s/it, loss=1.446, v_num=0]#015Epoch 1:  19%|█▉        | 21/108 [00:37<02:36,  1.80s/it, loss=1.446, v_num=0]#015Epoch 1:  20%|██        | 22/108 [00:39<02:35,  1.81s/it, loss=1.446, v_num=0]#015Epoch 1:  20%|██        | 22/108 [00:39<02:35,  1.81s/it, loss=1.368, v_num=0]#015Epoch 1:  21%|██▏       | 23/108 [00:41<02:32,  1.80s/it, loss=1.368, v_num=0]#015Epoch 1:  21%|██▏       | 23/108 [00:41<02:32,  1.80s/it, loss=1.368, v_num=0]#015Epoch 1:  22%|██▏       | 24/108 [00:42<02:29,  1.78s/it, loss=1.368, v_num=0]#015Epoch 1:  22%|██▏       | 24/108 [00:42<02:29,  1.78s/it, loss=1.290, v_num=0]#015Epoch 1:  23%|██▎       | 25/108 [00:45<02:30,  1.81s/it, loss=1.290, v_num=0]#015Epoch 1:  23%|██▎       | 25/108 [00:45<02:30,  1.81s/it, loss=1.290, v_num=0]#015Epoch 1:  24%|██▍       | 26/108 [00:47<02:28,  1.81s/it, loss=1.290, v_num=0]#015Epoch 1:  24%|██▍       | 26/108 [00:47<02:28,  1.81s/it, loss=1.237, v_num=0]#015Epoch 1:  25%|██▌       | 27/108 [00:49<02:27,  1.82s/it, loss=1.237, v_num=0]#015Epoch 1:  25%|██▌       | 27/108 [00:49<02:27,  1.82s/it, loss=1.237, v_num=0]#015Epoch 1:  26%|██▌       | 28/108 [00:50<02:25,  1.82s/it, loss=1.237, v_num=0]#015Epoch 1:  26%|██▌       | 28/108 [00:50<02:25,  1.82s/it, loss=1.179, v_num=0]#015Epoch 1:  27%|██▋       | 29/108 [00:51<02:21,  1.79s/it, loss=1.179, v_num=0]#015Epoch 1:  27%|██▋       | 29/108 [00:51<02:21,  1.79s/it, loss=1.179, v_num=0]#015Epoch 1:  28%|██▊       | 30/108 [00:54<02:21,  1.81s/it, loss=1.179, v_num=0]#015Epoch 1:  28%|██▊       | 30/108 [00:54<02:21,  1.81s/it, loss=1.125, v_num=0]#015Epoch 1:  29%|██▊       | 31/108 [00:55<02:19,  1.81s/it, loss=1.125, v_num=0]#015Epoch 1:  29%|██▊       | 31/108 [00:55<02:19,  1.81s/it, loss=1.125, v_num=0]#015Epoch 1:  30%|██▉       | 32/108 [00:57<02:16,  1.80s/it, loss=1.125, v_num=0]#015Epoch 1:  30%|██▉       | 32/108 [00:57<02:16,  1.80s/it, loss=1.076, v_num=0]#015Epoch 1:  31%|███       | 33/108 [00:59<02:14,  1.80s/it, loss=1.076, v_num=0]#015Epoch 1:  31%|███       | 33/108 [00:59<02:14,  1.80s/it, loss=1.076, v_num=0]#015Epoch 1:  31%|███▏      | 34/108 [01:00<02:12,  1.79s/it, loss=1.076, v_num=0]#015Epoch 1:  31%|███▏      | 34/108 [01:00<02:12,  1.79s/it, loss=1.032, v_num=0]#015Epoch 1:  32%|███▏      | 35/108 [01:02<02:09,  1.78s/it, loss=1.032, v_num=0]#015Epoch 1:  32%|███▏      | 35/108 [01:02<02:09,  1.78s/it, loss=1.032, v_num=0]#015Epoch 1:  33%|███▎      | 36/108 [01:03<02:07,  1.77s/it, loss=1.032, v_num=0]#015Epoch 1:  33%|███▎      | 36/108 [01:03<02:07,  1.77s/it, loss=0.996, v_num=0]#015Epoch 1:  34%|███▍      | 37/108 [01:05<02:05,  1.76s/it, loss=0.996, v_num=0]#015Epoch 1:  34%|███▍      | 37/108 [01:05<02:05,  1.76s/it, loss=0.996, v_num=0]#015Epoch 1:  35%|███▌      | 38/108 [01:06<02:03,  1.76s/it, loss=0.996, v_num=0]#015Epoch 1:  35%|███▌      | 38/108 [01:06<02:03,  1.76s/it, loss=0.956, v_num=0]#015Epoch 1:  36%|███▌      | 39/108 [01:08<02:01,  1.75s/it, loss=0.956, v_num=0]#015Epoch 1:  36%|███▌      | 39/108 [01:08<02:01,  1.75s/it, loss=0.956, v_num=0]#015Epoch 1:  37%|███▋      | 40/108 [01:10<01:59,  1.75s/it, loss=0.956, v_num=0]#015Epoch 1:  37%|███▋      | 40/108 [01:10<01:59,  1.75s/it, loss=0.923, v_num=0]#015Epoch 1:  38%|███▊      | 41/108 [01:11<01:56,  1.73s/it, loss=0.923, v_num=0]#015Epoch 1:  38%|███▊      | 41/108 [01:11<01:56,  1.73s/it, loss=0.923, v_num=0]#015Epoch 1:  39%|███▉      | 42/108 [01:12<01:53,  1.72s/it, loss=0.923, v_num=0]#015Epoch 1:  39%|███▉      | 42/108 [01:12<01:53,  1.72s/it, loss=0.807, v_num=0]#015Epoch 1:  40%|███▉      | 43/108 [01:13<01:51,  1.72s/it, loss=0.807, v_num=0]#015Epoch 1:  40%|███▉      | 43/108 [01:13<01:51,  1.72s/it, loss=0.807, v_num=0]#015Epoch 1:  41%|████      | 44/108 [01:15<01:49,  1.71s/it, loss=0.807, v_num=0]#015Epoch 1:  41%|████      | 44/108 [01:15<01:49,  1.71s/it, loss=0.698, v_num=0]#015Epoch 1:  42%|████▏     | 45/108 [01:16<01:47,  1.70s/it, loss=0.698, v_num=0]#015Epoch 1:  42%|████▏     | 45/108 [01:16<01:47,  1.70s/it, loss=0.698, v_num=0]#015Epoch 1:  43%|████▎     | 46/108 [01:17<01:45,  1.69s/it, loss=0.698, v_num=0]#015Epoch 1:  43%|████▎     | 46/108 [01:17<01:45,  1.69s/it, loss=0.628, v_num=0]#015Epoch 1:  44%|████▎     | 47/108 [01:18<01:42,  1.68s/it, loss=0.628, v_num=0]#015Epoch 1:  44%|████▎     | 47/108 [01:18<01:42,  1.68s/it, loss=0.628, v_num=0]#015Epoch 1:  44%|████▍     | 48/108 [01:20<01:40,  1.68s/it, loss=0.628, v_num=0]#015Epoch 1:  44%|████▍     | 48/108 [01:20<01:40,  1.68s/it, loss=0.545, v_num=0]#015Epoch 1:  45%|████▌     | 49/108 [01:22<01:39,  1.68s/it, loss=0.545, v_num=0]#015Epoch 1:  45%|████▌     | 49/108 [01:22<01:39,  1.68s/it, loss=0.545, v_num=0]#015Epoch 1:  46%|████▋     | 50/108 [01:23<01:36,  1.67s/it, loss=0.545, v_num=0]#015Epoch 1:  46%|████▋     | 50/108 [01:23<01:36,  1.67s/it, loss=0.486, v_num=0]#015Epoch 1:  47%|████▋     | 51/108 [01:24<01:34,  1.67s/it, loss=0.486, v_num=0]#015Epoch 1:  47%|████▋     | 51/108 [01:24<01:34,  1.67s/it, loss=0.486, v_num=0]#015Epoch 1:  48%|████▊     | 52/108 [01:26<01:32,  1.66s/it, loss=0.486, v_num=0]#015Epoch 1:  48%|████▊     | 52/108 [01:26<01:32,  1.66s/it, loss=0.442, v_num=0]#015Epoch 1:  49%|████▉     | 53/108 [01:27<01:30,  1.65s/it, loss=0.442, v_num=0]#015Epoch 1:  49%|████▉     | 53/108 [01:27<01:30,  1.65s/it, loss=0.442, v_num=0]#015Epoch 1:  50%|█████     | 54/108 [01:28<01:28,  1.64s/it, loss=0.442, v_num=0]#015Epoch 1:  50%|█████     | 54/108 [01:28<01:28,  1.64s/it, loss=0.400, v_num=0]#015Epoch 1:  51%|█████     | 55/108 [01:30<01:26,  1.64s/it, loss=0.400, v_num=0]#015Epoch 1:  51%|█████     | 55/108 [01:30<01:26,  1.64s/it, loss=0.400, v_num=0]#015Epoch 1:  52%|█████▏    | 56/108 [01:31<01:24,  1.63s/it, loss=0.400, v_num=0]#015Epoch 1:  52%|█████▏    | 56/108 [01:31<01:24,  1.63s/it, loss=0.372, v_num=0]#015Epoch 1:  53%|█████▎    | 57/108 [01:32<01:22,  1.62s/it, loss=0.372, v_num=0]#015Epoch 1:  53%|█████▎    | 57/108 [01:32<01:22,  1.62s/it, loss=0.372, v_num=0]#015Epoch 1:  54%|█████▎    | 58/108 [01:34<01:21,  1.63s/it, loss=0.372, v_num=0]#015Epoch 1:  54%|█████▎    | 58/108 [01:34<01:21,  1.63s/it, loss=0.342, v_num=0]#015Epoch 1:  55%|█████▍    | 59/108 [01:35<01:19,  1.62s/it, loss=0.342, v_num=0]#015Epoch 1:  55%|█████▍    | 59/108 [01:35<01:19,  1.62s/it, loss=0.342, v_num=0]#015Epoch 1:  56%|█████▌    | 60/108 [01:36<01:17,  1.61s/it, loss=0.342, v_num=0]#015Epoch 1:  56%|█████▌    | 60/108 [01:36<01:17,  1.61s/it, loss=0.313, v_num=0]#015Epoch 1:  56%|█████▋    | 61/108 [01:38<01:15,  1.61s/it, loss=0.313, v_num=0]#015Epoch 1:  56%|█████▋    | 61/108 [01:38<01:15,  1.61s/it, loss=0.313, v_num=0]#015Epoch 1:  57%|█████▋    | 62/108 [01:39<01:13,  1.60s/it, loss=0.313, v_num=0]#015Epoch 1:  57%|█████▋    | 62/108 [01:39<01:13,  1.60s/it, loss=0.293, v_num=0]#015Epoch 1:  58%|█████▊    | 63/108 [01:41<01:12,  1.61s/it, loss=0.293, v_num=0]#015Epoch 1:  58%|█████▊    | 63/108 [01:41<01:12,  1.61s/it, loss=0.293, v_num=0]#015Epoch 1:  59%|█████▉    | 64/108 [01:43<01:10,  1.61s/it, loss=0.293, v_num=0]#015Epoch 1:  59%|█████▉    | 64/108 [01:43<01:10,  1.61s/it, loss=0.280, v_num=0]#015Epoch 1:  60%|██████    | 65/108 [01:44<01:08,  1.60s/it, loss=0.280, v_num=0]#015Epoch 1:  60%|██████    | 65/108 [01:44<01:08,  1.60s/it, loss=0.280, v_num=0]#015Epoch 1:  61%|██████    | 66/108 [01:46<01:07,  1.61s/it, loss=0.280, v_num=0]#015Epoch 1:  61%|██████    | 66/108 [01:46<01:07,  1.61s/it, loss=0.257, v_num=0]#015Epoch 1:  62%|██████▏   | 67/108 [01:47<01:05,  1.60s/it, loss=0.257, v_num=0]#015Epoch 1:  62%|██████▏   | 67/108 [01:47<01:05,  1.60s/it, loss=0.257, v_num=0]#015Epoch 1:  63%|██████▎   | 68/108 [01:48<01:03,  1.60s/it, loss=0.257, v_num=0]#015Epoch 1:  63%|██████▎   | 68/108 [01:48<01:03,  1.60s/it, loss=0.241, v_num=0]#015Epoch 1:  64%|██████▍   | 69/108 [01:50<01:02,  1.60s/it, loss=0.241, v_num=0]#015Epoch 1:  64%|██████▍   | 69/108 [01:50<01:02,  1.60s/it, loss=0.241, v_num=0]#015Epoch 1:  65%|██████▍   | 70/108 [01:51<01:00,  1.60s/it, loss=0.241, v_num=0]#015Epoch 1:  65%|██████▍   | 70/108 [01:51<01:00,  1.60s/it, loss=0.230, v_num=0]#015Epoch 1:  66%|██████▌   | 71/108 [01:53<00:59,  1.60s/it, loss=0.230, v_num=0]#015Epoch 1:  66%|██████▌   | 71/108 [01:53<00:59,  1.60s/it, loss=0.230, v_num=0]#015Epoch 1:  67%|██████▋   | 72/108 [01:54<00:57,  1.59s/it, loss=0.230, v_num=0]#015Epoch 1:  67%|██████▋   | 72/108 [01:54<00:57,  1.59s/it, loss=0.221, v_num=0]#015Epoch 1:  68%|██████▊   | 73/108 [01:56<00:55,  1.60s/it, loss=0.221, v_num=0]#015Epoch 1:  68%|██████▊   | 73/108 [01:56<00:55,  1.60s/it, loss=0.221, v_num=0]#015Epoch 1:  69%|██████▊   | 74/108 [01:58<00:54,  1.60s/it, loss=0.221, v_num=0]#015Epoch 1:  69%|██████▊   | 74/108 [01:58<00:54,  1.60s/it, loss=0.217, v_num=0]#015Epoch 1:  69%|██████▉   | 75/108 [01:59<00:52,  1.60s/it, loss=0.217, v_num=0]#015Epoch 1:  69%|██████▉   | 75/108 [01:59<00:52,  1.60s/it, loss=0.217, v_num=0]#015Epoch 1:  70%|███████   | 76/108 [02:01<00:51,  1.60s/it, loss=0.217, v_num=0]#015Epoch 1:  70%|███████   | 76/108 [02:01<00:51,  1.60s/it, loss=0.218, v_num=0]#015Epoch 1:  71%|███████▏  | 77/108 [02:02<00:49,  1.60s/it, loss=0.218, v_num=0]#015Epoch 1:  71%|███████▏  | 77/108 [02:02<00:49,  1.60s/it, loss=0.218, v_num=0]#015Epoch 1:  72%|███████▏  | 78/108 [02:04<00:47,  1.60s/it, loss=0.218, v_num=0]#015Epoch 1:  72%|███████▏  | 78/108 [02:04<00:47,  1.60s/it, loss=0.218, v_num=0]#015Epoch 1:  73%|███████▎  | 79/108 [02:05<00:46,  1.59s/it, loss=0.218, v_num=0]#015Epoch 1:  73%|███████▎  | 79/108 [02:05<00:46,  1.59s/it, loss=0.218, v_num=0]#015Epoch 1:  74%|███████▍  | 80/108 [02:07<00:44,  1.59s/it, loss=0.218, v_num=0]#015Epoch 1:  74%|███████▍  | 80/108 [02:07<00:44,  1.59s/it, loss=0.214, v_num=0]#015Epoch 1:  75%|███████▌  | 81/108 [02:08<00:42,  1.58s/it, loss=0.214, v_num=0]#015Epoch 1:  75%|███████▌  | 81/108 [02:08<00:42,  1.58s/it, loss=0.214, v_num=0]#015Epoch 1:  76%|███████▌  | 82/108 [02:09<00:41,  1.58s/it, loss=0.214, v_num=0]#015Epoch 1:  76%|███████▌  | 82/108 [02:09<00:41,  1.58s/it, loss=0.205, v_num=0]#015Epoch 1:  77%|███████▋  | 83/108 [02:10<00:39,  1.58s/it, loss=0.205, v_num=0]#015Epoch 1:  77%|███████▋  | 83/108 [02:10<00:39,  1.58s/it, loss=0.205, v_num=0]#015Epoch 1:  78%|███████▊  | 84/108 [02:12<00:37,  1.57s/it, loss=0.205, v_num=0]#015Epoch 1:  78%|███████▊  | 84/108 [02:12<00:37,  1.57s/it, loss=0.201, v_num=0]#015Epoch 1:  79%|███████▊  | 85/108 [02:13<00:36,  1.57s/it, loss=0.201, v_num=0]#015Epoch 1:  79%|███████▊  | 85/108 [02:13<00:36,  1.57s/it, loss=0.201, v_num=0]#015Epoch 1:  80%|███████▉  | 86/108 [02:15<00:34,  1.57s/it, loss=0.201, v_num=0]#015Epoch 1:  80%|███████▉  | 86/108 [02:15<00:34,  1.57s/it, loss=0.191, v_num=0]#015Epoch 1:  81%|████████  | 87/108 [02:16<00:32,  1.57s/it, loss=0.191, v_num=0]#015Epoch 1:  81%|████████  | 87/108 [02:16<00:32,  1.57s/it, loss=0.191, v_num=0]#015Epoch 1:  81%|████████▏ | 88/108 [02:17<00:31,  1.56s/it, loss=0.191, v_num=0]#015Epoch 1:  81%|████████▏ | 88/108 [02:17<00:31,  1.56s/it, loss=0.207, v_num=0]#015Epoch 1:  82%|████████▏ | 89/108 [02:18<00:29,  1.56s/it, loss=0.207, v_num=0]#015Epoch 1:  82%|████████▏ | 89/108 [02:18<00:29,  1.56s/it, loss=0.207, v_num=0]#015Epoch 1:  83%|████████▎ | 90/108 [02:20<00:28,  1.56s/it, loss=0.207, v_num=0]#015Epoch 1:  83%|████████▎ | 90/108 [02:20<00:28,  1.56s/it, loss=0.214, v_num=0]#015Epoch 1:  84%|████████▍ | 91/108 [02:21<00:26,  1.55s/it, loss=0.214, v_num=0]#015Epoch 1:  84%|████████▍ | 91/108 [02:21<00:26,  1.55s/it, loss=0.214, v_num=0]#015Epoch 1:  85%|████████▌ | 92/108 [02:22<00:24,  1.55s/it, loss=0.214, v_num=0]#015Epoch 1:  85%|████████▌ | 92/108 [02:22<00:24,  1.55s/it, loss=0.209, v_num=0]#015Epoch 1:  86%|██████�\u001b[0m\n",
      "\u001b[34m�█▌ | 93/108 [02:24<00:23,  1.55s/it, loss=0.209, v_num=0]#015Epoch 1:  86%|████████▌ | 93/108 [02:24<00:23,  1.55s/it, loss=0.209, v_num=0]#015Epoch 1:  87%|████████▋ | 94/108 [02:25<00:21,  1.55s/it, loss=0.209, v_num=0]#015Epoch 1:  87%|████████▋ | 94/108 [02:25<00:21,  1.55s/it, loss=0.213, v_num=0]#015Epoch 1:  88%|████████▊ | 95/108 [02:27<00:20,  1.55s/it, loss=0.213, v_num=0]#015Epoch 1:  88%|████████▊ | 95/108 [02:27<00:20,  1.55s/it, loss=0.213, v_num=0]#015Epoch 1:  89%|████████▉ | 96/108 [02:28<00:18,  1.55s/it, loss=0.213, v_num=0]#015Epoch 1:  89%|████████▉ | 96/108 [02:28<00:18,  1.55s/it, loss=0.208, v_num=0]#015Epoch 1:  90%|████████▉ | 97/108 [02:29<00:16,  1.54s/it, loss=0.208, v_num=0]#015Epoch 1:  90%|████████▉ | 97/108 [02:29<00:16,  1.54s/it, loss=0.208, v_num=0]#015Epoch 1:  91%|█████████ | 98/108 [02:31<00:15,  1.55s/it, loss=0.208, v_num=0]#015Epoch 1:  91%|█████████ | 98/108 [02:31<00:15,  1.55s/it, loss=0.206, v_num=0]#015Epoch 1:  92%|█████████▏| 99/108 [02:33<00:13,  1.55s/it, loss=0.206, v_num=0]#015Epoch 1:  92%|█████████▏| 99/108 [02:33<00:13,  1.55s/it, loss=0.206, v_num=0]#015Epoch 1:  93%|█████████▎| 100/108 [02:34<00:12,  1.54s/it, loss=0.206, v_num=0]#015Epoch 1:  93%|█████████▎| 100/108 [02:34<00:12,  1.54s/it, loss=0.203, v_num=0]#015Epoch 1:  94%|█████████▎| 101/108 [02:35<00:10,  1.54s/it, loss=0.203, v_num=0]#015Epoch 1:  94%|█████████▎| 101/108 [02:35<00:10,  1.54s/it, loss=0.203, v_num=0]#015Epoch 1:  94%|█████████▍| 102/108 [02:37<00:09,  1.54s/it, loss=0.203, v_num=0]#015Epoch 1:  94%|█████████▍| 102/108 [02:37<00:09,  1.54s/it, loss=0.202, v_num=0]#015Epoch 1:  95%|█████████▌| 103/108 [02:38<00:07,  1.54s/it, loss=0.202, v_num=0]#015Epoch 1:  95%|█████████▌| 103/108 [02:38<00:07,  1.54s/it, loss=0.202, v_num=0]#015Epoch 1:  96%|█████████▋| 104/108 [02:40<00:06,  1.54s/it, loss=0.202, v_num=0]#015Epoch 1:  96%|█████████▋| 104/108 [02:40<00:06,  1.54s/it, loss=0.204, v_num=0]#015Epoch 1:  97%|█████████▋| 105/108 [02:41<00:04,  1.54s/it, loss=0.204, v_num=0]#015Epoch 1:  97%|█████████▋| 105/108 [02:41<00:04,  1.54s/it, loss=0.204, v_num=0]#015Epoch 1:  98%|█████████▊| 106/108 [02:42<00:03,  1.54s/it, loss=0.204, v_num=0]#015Epoch 1:  98%|█████████▊| 106/108 [02:42<00:03,  1.54s/it, loss=0.205, v_num=0]#015Epoch 1:  99%|█████████▉| 107/108 [02:44<00:01,  1.54s/it, loss=0.205, v_num=0]#015Epoch 1:  99%|█████████▉| 107/108 [02:44<00:01,  1.54s/it, loss=0.205, v_num=0]\u001b[0m\n",
      "\u001b[34mValidating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidating: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]#033[A#015Epoch 1: 100%|██████████| 108/108 [02:44<00:00,  1.52s/it, loss=0.205, v_num=0]INFO:__main__:***** Validation results *****\u001b[0m\n",
      "\u001b[34mINFO:__main__:avg_val_loss = tensor(0.1042, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:loss = tensor(0.4639, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:train_loss = tensor(0.4639, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:val_loss = tensor(0.1042, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 108/108 [02:44<00:00,  1.52s/it, loss=0.205, v_num=0, val_loss=0.104]\u001b[0m\n",
      "\n",
      "2022-01-10 07:42:49 Uploading - Uploading generated training model\u001b[35mINFO:__main__:***** Validation results *****\u001b[0m\n",
      "\u001b[35mINFO:__main__:avg_train_loss = tensor(0.9223, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:avg_val_loss = tensor(0.2338, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:loss = tensor(0.1692, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:train_loss = tensor(0.1692, device='cuda:0')\u001b[0m\n",
      "\u001b[35mINFO:__main__:val_loss = tensor(0.2338, device='cuda:0')\u001b[0m\n",
      "\u001b[34m#033[A#015Epoch 1:   0%|          | 0/108 [00:00<?, ?it/s, loss=0.205, v_num=0, val_loss=0.104]          #015Epoch 2:   0%|          | 0/108 [00:00<?, ?it/s, loss=0.205, v_num=0, val_loss=0.104]#015Epoch 2:   1%|          | 1/108 [00:01<02:26,  1.37s/it, loss=0.205, v_num=0, val_loss=0.104]#015Epoch 2:   1%|          | 1/108 [00:01<02:26,  1.37s/it, loss=0.205, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   2%|▏         | 2/108 [00:02<02:17,  1.30s/it, loss=0.205, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   2%|▏         | 2/108 [00:02<02:17,  1.30s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   3%|▎         | 3/108 [00:04<02:24,  1.38s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   3%|▎         | 3/108 [00:04<02:24,  1.38s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   4%|▎         | 4/108 [00:05<02:27,  1.42s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   4%|▎         | 4/108 [00:05<02:27,  1.42s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   5%|▍         | 5/108 [00:06<02:20,  1.36s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   5%|▍         | 5/108 [00:06<02:20,  1.36s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   6%|▌         | 6/108 [00:08<02:23,  1.40s/it, loss=0.208, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   6%|▌         | 6/108 [00:08<02:23,  1.40s/it, loss=0.207, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   6%|▋         | 7/108 [00:09<02:18,  1.37s/it, loss=0.207, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   6%|▋         | 7/108 [00:09<02:18,  1.37s/it, loss=0.207, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   7%|▋         | 8/108 [00:11<02:21,  1.42s/it, loss=0.207, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   7%|▋         | 8/108 [00:11<02:21,  1.42s/it, loss=0.202, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   8%|▊         | 9/108 [00:12<02:20,  1.42s/it, loss=0.202, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   8%|▊         | 9/108 [00:12<02:20,  1.42s/it, loss=0.202, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   9%|▉         | 10/108 [00:14<02:18,  1.42s/it, loss=0.202, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:   9%|▉         | 10/108 [00:14<02:18,  1.42s/it, loss=0.187, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  10%|█         | 11/108 [00:15<02:18,  1.42s/it, loss=0.187, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  10%|█         | 11/108 [00:15<02:18,  1.43s/it, loss=0.187, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  11%|█         | 12/108 [00:17<02:21,  1.47s/it, loss=0.187, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  11%|█         | 12/108 [00:17<02:21,  1.47s/it, loss=0.185, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  12%|█▏        | 13/108 [00:18<02:18,  1.45s/it, loss=0.185, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  12%|█▏        | 13/108 [00:18<02:18,  1.45s/it, loss=0.185, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  13%|█▎        | 14/108 [00:20<02:17,  1.46s/it, loss=0.185, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  13%|█▎        | 14/108 [00:20<02:17,  1.46s/it, loss=0.179, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  14%|█▍        | 15/108 [00:21<02:13,  1.44s/it, loss=0.179, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  14%|█▍        | 15/108 [00:21<02:13,  1.44s/it, loss=0.179, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  15%|█▍        | 16/108 [00:22<02:10,  1.42s/it, loss=0.179, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  15%|█▍        | 16/108 [00:22<02:10,  1.42s/it, loss=0.177, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  16%|█▌        | 17/108 [00:23<02:07,  1.40s/it, loss=0.177, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  16%|█▌        | 17/108 [00:23<02:07,  1.40s/it, loss=0.177, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  17%|█▋        | 18/108 [00:25<02:05,  1.40s/it, loss=0.177, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  17%|█▋        | 18/108 [00:25<02:05,  1.40s/it, loss=0.178, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  18%|█▊        | 19/108 [00:26<02:04,  1.40s/it, loss=0.178, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  18%|█▊        | 19/108 [00:26<02:04,  1.40s/it, loss=0.178, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  19%|█▊        | 20/108 [00:28<02:05,  1.42s/it, loss=0.178, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  19%|█▊        | 20/108 [00:28<02:05,  1.42s/it, loss=0.182, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  19%|█▉        | 21/108 [00:30<02:05,  1.44s/it, loss=0.182, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  19%|█▉        | 21/108 [00:30<02:05,  1.44s/it, loss=0.182, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  20%|██        | 22/108 [00:31<02:04,  1.45s/it, loss=0.182, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  20%|██        | 22/108 [00:31<02:04,  1.45s/it, loss=0.172, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  21%|██▏       | 23/108 [00:33<02:02,  1.44s/it, loss=0.172, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  21%|██▏       | 23/108 [00:33<02:02,  1.44s/it, loss=0.172, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  22%|██▏       | 24/108 [00:34<02:01,  1.44s/it, loss=0.172, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  22%|██▏       | 24/108 [00:34<02:01,  1.44s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  23%|██▎       | 25/108 [00:36<02:00,  1.45s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  23%|██▎       | 25/108 [00:36<02:00,  1.45s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  24%|██▍       | 26/108 [00:37<01:58,  1.44s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  24%|██▍       | 26/108 [00:37<01:58,  1.44s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  25%|██▌       | 27/108 [00:38<01:56,  1.44s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  25%|██▌       | 27/108 [00:38<01:56,  1.44s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  26%|██▌       | 28/108 [00:40<01:54,  1.43s/it, loss=0.163, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  26%|██▌       | 28/108 [00:40<01:54,  1.43s/it, loss=0.161, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  27%|██▋       | 29/108 [00:41<01:52,  1.43s/it, loss=0.161, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  27%|██▋       | 29/108 [00:41<01:52,  1.43s/it, loss=0.161, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  28%|██▊       | 30/108 [00:42<01:51,  1.43s/it, loss=0.161, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  28%|██▊       | 30/108 [00:42<01:51,  1.43s/it, loss=0.160, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  29%|██▊       | 31/108 [00:44<01:49,  1.42s/it, loss=0.160, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  29%|██▊       | 31/108 [00:44<01:49,  1.42s/it, loss=0.160, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  30%|██▉       | 32/108 [00:45<01:47,  1.42s/it, loss=0.160, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  30%|██▉       | 32/108 [00:45<01:47,  1.42s/it, loss=0.155, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  31%|███       | 33/108 [00:46<01:45,  1.41s/it, loss=0.155, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  31%|███       | 33/108 [00:46<01:45,  1.41s/it, loss=0.155, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  31%|███▏      | 34/108 [00:47<01:43,  1.40s/it, loss=0.155, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  31%|███▏      | 34/108 [00:47<01:43,  1.40s/it, loss=0.154, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  32%|███▏      | 35/108 [00:49<01:42,  1.40s/it, loss=0.154, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  32%|███▏      | 35/108 [00:49<01:42,  1.40s/it, loss=0.154, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  33%|███▎      | 36/108 [00:50<01:41,  1.41s/it, loss=0.154, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  33%|███▎      | 36/108 [00:50<01:41,  1.41s/it, loss=0.150, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  34%|███▍      | 37/108 [00:51<01:39,  1.40s/it, loss=0.150, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  34%|███▍      | 37/108 [00:51<01:39,  1.40s/it, loss=0.150, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  35%|███▌      | 38/108 [00:53<01:37,  1.40s/it, loss=0.150, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  35%|███▌      | 38/108 [00:53<01:37,  1.40s/it, loss=0.147, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  36%|███▌      | 39/108 [00:55<01:38,  1.42s/it, loss=0.147, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  36%|███▌      | 39/108 [00:55<01:38,  1.42s/it, loss=0.147, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  37%|███▋      | 40/108 [00:57<01:36,  1.43s/it, loss=0.147, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  37%|███▋      | 40/108 [00:57<01:36,  1.43s/it, loss=0.148, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  38%|███▊      | 41/108 [00:59<01:36,  1.44s/it, loss=0.148, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  38%|███▊      | 41/108 [00:59<01:36,  1.44s/it, loss=0.148, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  39%|███▉      | 42/108 [01:01<01:36,  1.46s/it, loss=0.148, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  39%|███▉      | 42/108 [01:01<01:36,  1.46s/it, loss=0.142, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  40%|███▉      | 43/108 [01:03<01:35,  1.47s/it, loss=0.142, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  40%|███▉      | 43/108 [01:03<01:35,  1.47s/it, loss=0.142, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  41%|████      | 44/108 [01:04<01:34,  1.47s/it, loss=0.142, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  41%|████      | 44/108 [01:04<01:34,  1.47s/it, loss=0.143, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  42%|████▏     | 45/108 [01:07<01:34,  1.49s/it, loss=0.143, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  42%|████▏     | 45/108 [01:07<01:34,  1.49s/it, loss=0.143, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  43%|████▎     | 46/108 [01:08<01:32,  1.49s/it, loss=0.143, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  43%|████▎     | 46/108 [01:08<01:32,  1.49s/it, loss=0.139, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  44%|████▎     | 47/108 [01:10<01:31,  1.50s/it, loss=0.139, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  44%|████▎     | 47/108 [01:10<01:31,  1.50s/it, loss=0.139, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  44%|████▍     | 48/108 [01:12<01:30,  1.51s/it, loss=0.139, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  44%|████▍     | 48/108 [01:12<01:30,  1.51s/it, loss=0.138, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  45%|████▌     | 49/108 [01:13<01:28,  1.51s/it, loss=0.138, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  45%|████▌     | 49/108 [01:13<01:28,  1.51s/it, loss=0.138, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  46%|████▋     | 50/108 [01:15<01:28,  1.52s/it, loss=0.138, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  46%|████▋     | 50/108 [01:15<01:28,  1.52s/it, loss=0.135, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  47%|████▋     | 51/108 [01:17<01:26,  1.52s/it, loss=0.135, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  47%|████▋     | 51/108 [01:17<01:26,  1.52s/it, loss=0.135, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  48%|████▊     | 52/108 [01:18<01:24,  1.52s/it, loss=0.135, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  48%|████▊     | 52/108 [01:18<01:25,  1.52s/it, loss=0.130, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  49%|████▉     | 53/108 [01:20<01:23,  1.51s/it, loss=0.130, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  49%|████▉     | 53/108 [01:20<01:23,  1.51s/it, loss=0.130, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  50%|█████     | 54/108 [01:22<01:22,  1.52s/it, loss=0.130, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  50%|█████     | 54/108 [01:22<01:22,  1.52s/it, loss=0.128, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  51%|█████     | 55/108 [01:23<01:20,  1.52s/it, loss=0.128, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  51%|█████     | 55/108 [01:23<01:20,  1.52s/it, loss=0.128, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  52%|█████▏    | 56/108 [01:25<01:19,  1.53s/it, loss=0.128, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  52%|█████▏    | 56/108 [01:25<01:19,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  53%|█████▎    | 57/108 [01:27<01:18,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  53%|█████▎    | 57/108 [01:27<01:18,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  54%|█████▎    | 58/108 [01:28<01:16,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  54%|█████▎    | 58/108 [01:28<01:16,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  55%|█████▍    | 59/108 [01:30<01:14,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  55%|█████▍    | 59/108 [01:30<01:14,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  56%|█████▌    | 60/108 [01:31<01:13,  1.53s/it, loss=0.126, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  56%|█████▌    | 60/108 [01:31<01:13,  1.53s/it, loss=0.125, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  56%|█████▋    | 61/108 [01:33<01:11,  1.52s/it, loss=0.125, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  56%|█████▋    | 61/108 [01:33<01:11,  1.52s/it, loss=0.125, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  57%|█████▋    | 62/108 [01:34<01:10,  1.53s/it, loss=0.125, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  57%|█████▋    | 62/108 [01:34<01:10,  1.53s/it, loss=0.117, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  58%|█████▊    | 63/108 [01:36<01:08,  1.53s/it, loss=0.117, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  58%|█████▊    | 63/108 [01:36<01:08,  1.53s/it, loss=0.117, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  59%|█████▉    | 64/108 [01:37<01:07,  1.53s/it, loss=0.117, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  59%|█████▉    | 64/108 [01:37<01:07,  1.53s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  60%|██████    | 65/108 [01:38<01:05,  1.52s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  60%|██████    | 65/108 [01:38<01:05,  1.52s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  61%|██████    | 66/108 [01:40<01:04,  1.52s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  61%|██████    | 66/108 [01:40<01:04,  1.52s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2: \n",
      " 62%|██████▏   | 67/108 [01:42<01:02,  1.53s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  62%|██████▏   | 67/108 [01:42<01:02,  1.53s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  63%|██████▎   | 68/108 [01:43<01:00,  1.52s/it, loss=0.121, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  63%|██████▎   | 68/108 [01:43<01:00,  1.52s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  64%|██████▍   | 69/108 [01:45<00:59,  1.53s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  64%|██████▍   | 69/108 [01:45<00:59,  1.53s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  65%|██████▍   | 70/108 [01:47<00:58,  1.53s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  65%|██████▍   | 70/108 [01:47<00:58,  1.53s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  66%|██████▌   | 71/108 [01:48<00:56,  1.53s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  66%|██████▌   | 71/108 [01:48<00:56,  1.53s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  67%|██████▋   | 72/108 [01:49<00:54,  1.52s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  67%|██████▋   | 72/108 [01:49<00:54,  1.52s/it, loss=0.106, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  68%|██████▊   | 73/108 [01:51<00:53,  1.52s/it, loss=0.106, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  68%|██████▊   | 73/108 [01:51<00:53,  1.52s/it, loss=0.106, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  69%|██████▊   | 74/108 [01:52<00:51,  1.52s/it, loss=0.106, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  69%|██████▊   | 74/108 [01:52<00:51,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  69%|██████▉   | 75/108 [01:54<00:50,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  69%|██████▉   | 75/108 [01:54<00:50,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  70%|███████   | 76/108 [01:55<00:48,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  70%|███████   | 76/108 [01:55<00:48,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  71%|███████▏  | 77/108 [01:57<00:47,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  71%|███████▏  | 77/108 [01:57<00:47,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  72%|███████▏  | 78/108 [01:59<00:45,  1.53s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  72%|███████▏  | 78/108 [01:59<00:45,  1.53s/it, loss=0.102, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  73%|███████▎  | 79/108 [02:00<00:44,  1.53s/it, loss=0.102, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  73%|███████▎  | 79/108 [02:00<00:44,  1.53s/it, loss=0.102, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  74%|███████▍  | 80/108 [02:01<00:42,  1.52s/it, loss=0.102, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  74%|███████▍  | 80/108 [02:01<00:42,  1.52s/it, loss=0.100, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  75%|███████▌  | 81/108 [02:03<00:41,  1.53s/it, loss=0.100, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  75%|███████▌  | 81/108 [02:03<00:41,  1.53s/it, loss=0.100, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  76%|███████▌  | 82/108 [02:05<00:39,  1.53s/it, loss=0.100, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  76%|███████▌  | 82/108 [02:05<00:39,  1.53s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  77%|███████▋  | 83/108 [02:06<00:38,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  77%|███████▋  | 83/108 [02:06<00:38,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  78%|███████▊  | 84/108 [02:07<00:36,  1.52s/it, loss=0.105, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  78%|███████▊  | 84/108 [02:07<00:36,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  79%|███████▊  | 85/108 [02:09<00:34,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  79%|███████▊  | 85/108 [02:09<00:34,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  80%|███████▉  | 86/108 [02:10<00:33,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  80%|███████▉  | 86/108 [02:10<00:33,  1.52s/it, loss=0.101, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  81%|████████  | 87/108 [02:12<00:31,  1.52s/it, loss=0.101, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  81%|████████  | 87/108 [02:12<00:31,  1.52s/it, loss=0.101, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  81%|████████▏ | 88/108 [02:13<00:30,  1.52s/it, loss=0.101, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  81%|████████▏ | 88/108 [02:13<00:30,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  82%|████████▏ | 89/108 [02:15<00:28,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  82%|████████▏ | 89/108 [02:15<00:28,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  83%|████████▎ | 90/108 [02:16<00:27,  1.52s/it, loss=0.103, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  83%|████████▎ | 90/108 [02:16<00:27,  1.52s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  84%|████████▍ | 91/108 [02:17<00:25,  1.52s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  84%|████████▍ | 91/108 [02:17<00:25,  1.52s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  85%|████████▌ | 92/108 [02:19<00:24,  1.52s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  85%|████████▌ | 92/108 [02:19<00:24,  1.52s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  86%|████████▌ | 93/108 [02:20<00:22,  1.52s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  86%|████████▌ | 93/108 [02:20<00:22,  1.52s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  87%|████████▋ | 94/108 [02:22<00:21,  1.51s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  87%|████████▋ | 94/108 [02:22<00:21,  1.51s/it, loss=0.115, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  88%|████████▊ | 95/108 [02:23<00:19,  1.51s/it, loss=0.115, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  88%|████████▊ | 95/108 [02:23<00:19,  1.51s/it, loss=0.115, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  89%|████████▉ | 96/108 [02:24<00:18,  1.51s/it, loss=0.115, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  89%|████████▉ | 96/108 [02:24<00:18,  1.51s/it, loss=0.118, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  90%|████████▉ | 97/108 [02:26<00:16,  1.51s/it, loss=0.118, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  90%|████████▉ | 97/108 [02:26<00:16,  1.51s/it, loss=0.118, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  91%|█████████ | 98/108 [02:28<00:15,  1.51s/it, loss=0.118, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  91%|█████████ | 98/108 [02:28<00:15,  1.51s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  92%|█████████▏| 99/108 [02:29<00:13,  1.51s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  92%|█████████▏| 99/108 [02:29<00:13,  1.51s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  93%|█████████▎| 100/108 [02:31<00:12,  1.51s/it, loss=0.114, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  93%|█████████▎| 100/108 [02:31<00:12,  1.51s/it, loss=0.112, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  94%|█████████▎| 101/108 [02:32<00:10,  1.51s/it, loss=0.112, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  94%|█████████▎| 101/108 [02:32<00:10,  1.51s/it, loss=0.112, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  94%|█████████▍| 102/108 [02:34<00:09,  1.51s/it, loss=0.112, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  94%|█████████▍| 102/108 [02:34<00:09,  1.51s/it, loss=0.113, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  95%|█████████▌| 103/108 [02:35<00:07,  1.51s/it, loss=0.113, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  95%|█████████▌| 103/108 [02:35<00:07,  1.51s/it, loss=0.113, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  96%|█████████▋| 104/108 [02:37<00:06,  1.51s/it, loss=0.113, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  96%|█████████▋| 104/108 [02:37<00:06,  1.51s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  97%|█████████▋| 105/108 [02:38<00:04,  1.51s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  97%|█████████▋| 105/108 [02:38<00:04,  1.51s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  98%|█████████▊| 106/108 [02:40<00:03,  1.51s/it, loss=0.107, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  98%|█████████▊| 106/108 [02:40<00:03,  1.51s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  99%|█████████▉| 107/108 [02:41<00:01,  1.51s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]#015Epoch 2:  99%|█████████▉| 107/108 [02:41<00:01,  1.51s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]\u001b[0m\n",
      "\u001b[34mValidating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidating: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]#033[A#015Epoch 2: 100%|██████████| 108/108 [02:41<00:00,  1.49s/it, loss=0.111, v_num=0, val_loss=0.104, avg_train_loss=0.95]INFO:__main__:***** Validation results *****\u001b[0m\n",
      "\u001b[34mINFO:__main__:avg_train_loss = tensor(0.9504, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:avg_val_loss = tensor(0.0137, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:epoch = 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:loss = tensor(0.1332, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:train_loss = tensor(0.1332, device='cuda:0')\u001b[0m\n",
      "\u001b[34mINFO:__main__:val_loss = tensor(0.0137, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 108/108 [02:41<00:00,  1.49s/it, loss=0.111, v_num=0, val_loss=0.0137, avg_train_loss=0.95]\u001b[0m\n",
      "\u001b[35mFinish training and saving the model!\u001b[0m\n",
      "\u001b[34m#033[A#015Epoch 2: 100%|██████████| 108/108 [02:44<00:00,  1.52s/it, loss=0.111, v_num=0, val_loss=0.0137, avg_train_loss=0.95]\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"G_ABSA_train.py\", line 279, in save_model\n",
      "    checkpoint_path = os.path.join(work_dir, last_model)\n",
      "  File \"/opt/conda/lib/python3.6/posixpath.py\", line 94, in join\n",
      "    genericpath._check_arg_types('join', a, *p)\n",
      "  File \"/opt/conda/lib/python3.6/genericpath.py\", line 149, in _check_arg_types\n",
      "    (funcname, s.__class__.__name__)) from None\u001b[0m\n",
      "\u001b[35mTypeError: join() argument must be str or bytes, not 'NoneType'\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"G_ABSA_train.py\", line 361, in <module>\n",
      "    save_model(args.output_dir, os.environ['SM_MODEL_DIR'])\n",
      "  File \"G_ABSA_train.py\", line 283, in save_model\n",
      "    print(f\"Exception when trying to copy {checkpoint_path} to {new_checkpoint_path}.\")\u001b[0m\n",
      "\u001b[35mUnboundLocalError: local variable 'checkpoint_path' referenced before assignment\u001b[0m\n",
      "\u001b[35m2022-01-10 07:42:46,602 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mCommand \"/opt/conda/bin/python3.6 G_ABSA_train.py --data_root /opt/ml/input/data/training --dataset rest16 --eval_batch_size 16 --gradient_accumulation_steps 2 --learning_rate 0.0003 --model_name_or_path t5-base --n_gpu 4 --nodes 2 --num_train_epochs 2 --out_dir /opt/ml/output --paradigm extraction --task tasd --train_batch_size 2\"\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"G_ABSA_train.py\", line 279, in save_model\n",
      "    checkpoint_path = os.path.join(work_dir, last_model)\n",
      "  File \"/opt/conda/lib/python3.6/posixpath.py\", line 94, in join\n",
      "    genericpath._check_arg_types('join', a, *p)\n",
      "  File \"/opt/conda/lib/python3.6/genericpath.py\", line 149, in _check_arg_types\n",
      "    (funcname, s.__class__.__name__)) from None\u001b[0m\n",
      "\u001b[35mTypeError: join() argument must be str or bytes, not 'NoneType'\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"G_ABSA_train.py\", line 361, in <module>\n",
      "    save_model(args.output_dir, os.environ['SM_MODEL_DIR'])\n",
      "  File \"G_ABSA_train.py\", line 283, in save_model\n",
      "    print(f\"Exception when trying to copy {checkpoint_path} to {new_checkpoint_path}.\")\u001b[0m\n",
      "\u001b[35mUnboundLocalError: local variable 'checkpoint_path' referenced before assignment\u001b[0m\n",
      "\u001b[34mFinish training and saving the model!\u001b[0m\n",
      "\u001b[34mModel config and checkpoints are saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34m2022-01-10 07:42:55,313 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-01-10 07:43:11 Failed - Training job failed\n",
      "ProfilerReport-1641799820: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job gabsa-training-2022-01-10-07-30-20-217: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 G_ABSA_train.py --data_root /opt/ml/input/data/training --dataset rest16 --eval_batch_size 16 --gradient_accumulation_steps 2 --learning_rate 0.0003 --model_name_or_path t5-base --n_gpu 4 --nodes 2 --num_train_epochs 2 --out_dir /opt/ml/output --paradigm extraction --task tasd --train_batch_size 2\"\nTraceback (most recent call last):\n  File \"G_ABSA_train.py\", line 279, in save_model\n    checkpoint_path = os.path.join(work_dir, last_model)\n  File \"/opt/conda/lib/python3.6/posixpath.py\", line 94, in join\n    genericpath._check_arg_types('join', a, *p)\n  File \"/opt/conda/lib/python3.6/genericpath.py\", line 149, in _check_arg_types\n    (funcname, s.__class__.__name__)) from None\nTypeError: join() argument must be str or bytes, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"G_ABSA_train.py\", line 361, in <module>\n    save_model(args.output_dir, os.environ['SM_MOD",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-019191943125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"s3://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/data/\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3779\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3336\u001b[0m                 ),\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m             )\n\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job gabsa-training-2022-01-10-07-30-20-217: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 G_ABSA_train.py --data_root /opt/ml/input/data/training --dataset rest16 --eval_batch_size 16 --gradient_accumulation_steps 2 --learning_rate 0.0003 --model_name_or_path t5-base --n_gpu 4 --nodes 2 --num_train_epochs 2 --out_dir /opt/ml/output --paradigm extraction --task tasd --train_batch_size 2\"\nTraceback (most recent call last):\n  File \"G_ABSA_train.py\", line 279, in save_model\n    checkpoint_path = os.path.join(work_dir, last_model)\n  File \"/opt/conda/lib/python3.6/posixpath.py\", line 94, in join\n    genericpath._check_arg_types('join', a, *p)\n  File \"/opt/conda/lib/python3.6/genericpath.py\", line 149, in _check_arg_types\n    (funcname, s.__class__.__name__)) from None\nTypeError: join() argument must be str or bytes, not 'NoneType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"G_ABSA_train.py\", line 361, in <module>\n    save_model(args.output_dir, os.environ['SM_MOD"
     ]
    }
   ],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                          role=role,\n",
    "                                          train_instance_count=2,\n",
    "                                          train_instance_type='ml.p3.8xlarge',\n",
    "                                          train_volume_size=100,\n",
    "                                          output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                          hyperparameters = hyperparameters, \n",
    "                                          sagemaker_session=session\n",
    ")\n",
    "\n",
    "est.fit({\"training\" : \"s3://\"+bucket+\"/data/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08181b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  7 10:02:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   44C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   38C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   38C    P0    40W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   42C    P0    40W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61837503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
