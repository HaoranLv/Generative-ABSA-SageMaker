<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AWS Datalab-细粒度情感分析 on SpotBot Workshop</title><link>/</link><description>Recent content in AWS Datalab-细粒度情感分析 on SpotBot Workshop</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 05 Nov 2021 14:52:27 +0800</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>1.1 算法概述</title><link>/01introduction/100algorithm.html</link><pubDate>Fri, 05 Nov 2021 14:52:27 +0800</pubDate><guid>/01introduction/100algorithm.html</guid><description>使用场景 根据生成方式可以分为生成式摘要和抽取式摘要。
抽取式摘要：找到一个文档中最重要的几个句子并对其进行拼接。 生成式摘要：是一个序列生成问题，通过源文档序列, 生成序列摘要序列 本workshop主要覆盖以下几个算法，对比结果如下
Pegasus BART</description></item><item><title>1.2 数据集</title><link>/01introduction/200data.html</link><pubDate>Fri, 05 Nov 2021 14:52:27 +0800</pubDate><guid>/01introduction/200data.html</guid><description>1.公开数据集 (英文) XSUM 227k BBC articles CNN/Dailymail，93k articles from the CNN, 220k articles from the Daily Mail NEWSROOM，1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications Multi-News，56k pairs of news articles and their human-written summaries from the http://sitenewser.com Gigaword，4M examples extracted from news articles，the task is to generate theheadline from the first sentence arXiv, PubMed，two long documentdatasets of scientific publications from http://arXiv.org (113k) andPubMed (215k).</description></item><item><title>1.3 评估指标</title><link>/01introduction/300metrics.html</link><pubDate>Fri, 05 Nov 2021 14:52:27 +0800</pubDate><guid>/01introduction/300metrics.html</guid><description>文本生成目前的一大瓶颈是如何客观，准确的评价机器生成文本的质量。自动文档摘要评价方法大致分为两类：
（1）内部评价方法：提供参考摘要，以参考摘要为基准评价系统摘要的质量。系统摘要与参考摘要越吻合，质量越高。
（2）外部评价方法：不提供参考摘要，利用文档摘要代替原文档执行某个文档相关的应用。例如：文档检索、文档分类等，能够提高应用性能的摘要被认为是质量好的摘要。
下面介绍两种比较简单的，经常用到的内部评价方法：
Edmundson 适于抽取式文本摘要，比较机械文摘(自动文摘系统得到的文摘)与目标文摘(从原文中抽取的句子)的句子重合率的高低对系统摘要进行评价。
计算公式：
重合率p = 匹配句子数/专家文摘句子数*100% 每一个机械文摘的重合率为按三个专家给出的文摘得到的重合率的平均值（其中，pi为相对于第i个专家的重合率，n为专家文摘总数）： ROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）基于摘要中n-gram的共现信息评价摘要，是一种面向n元词召回率的评价方法。 其中，Ref summaries表示标准摘要，count_match(n-gram)表示生成摘要和标准摘要中同时出现n-gram的个数，count(n-gram)表示参考摘要中出现的n-gram个数。</description></item><item><title>抽取式ABSA模型训练</title><link>/02extraction/02train/_index-1.html</link><pubDate>Fri, 05 Nov 2021 14:52:27 +0800</pubDate><guid>/02extraction/02train/_index-1.html</guid><description>paper - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence&amp;quot; (NAACL 2019)。 这个方法的输入输出设计很符合TABSA任务，做法简单，适合作为此类任务的baseline.
我们现在会用sagemaker进行一个模型的本地训练，使用ml.p3.8xlarge机型。
数据准备 首先下载代码
source activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/ABSA-BERT-pair.git 将数据data1119.csv上传到data/custom/data1119.csv
模型训练 接下来我们运行训练，首先下载预训练模型并转换为torch版本
cd ABSA-BERT-pair wget -P ./source/bert/pretrain_model/cn https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip cd ./source/bert/pretrain_model/cn unzip chinese_L-12_H-768_A-12.zip pip install tensorflow==1.13.1 cd /home/ec2-user/SageMaker/ABSA-BERT-pair/ python convert_tf_checkpoint_to_pytorch.py \ --tf_checkpoint_path ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_model.ckpt \ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \ --pytorch_dump_path ./source/bert/pretrain_model/cn/pytorch_model.bin 数据准备
cd generate/ python generate_custom_NLI_M.py 然后进行模型训练，演示目的，只训练一个epoch
cd ../ CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_TABSA-v1.py \ --task_name custom_NLI_M \ --data_dir data/custom/bert-pair/ \ --vocab_file .</description></item></channel></rss>