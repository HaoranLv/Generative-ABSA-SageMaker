<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>动手实验1: 基于Amazon SageMaker训练一个端到端抽取式的ABSA模型 on SpotBot Workshop</title><link>/02extraction.html</link><description>Recent content in 动手实验1: 基于Amazon SageMaker训练一个端到端抽取式的ABSA模型 on SpotBot Workshop</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 05 Nov 2021 14:52:27 +0800</lastBuildDate><atom:link href="/02extraction/index.xml" rel="self" type="application/rss+xml"/><item><title>抽取式ABSA模型训练</title><link>/02extraction/02train/_index-1.html</link><pubDate>Fri, 05 Nov 2021 14:52:27 +0800</pubDate><guid>/02extraction/02train/_index-1.html</guid><description>paper - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence&amp;quot; (NAACL 2019)。 这个方法的输入输出设计很符合TABSA任务，做法简单，适合作为此类任务的baseline.
我们现在会用sagemaker进行一个模型的本地训练，使用ml.p3.8xlarge机型。
数据准备 首先下载代码
source activate pytorch_p37 cd SageMaker git clone https://github.com/jackie930/ABSA-BERT-pair.git 将数据data1119.csv上传到data/custom/data1119.csv
模型训练 接下来我们运行训练，首先下载预训练模型并转换为torch版本
cd ABSA-BERT-pair wget -P ./source/bert/pretrain_model/cn https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip cd ./source/bert/pretrain_model/cn unzip chinese_L-12_H-768_A-12.zip pip install tensorflow==1.13.1 cd /home/ec2-user/SageMaker/ABSA-BERT-pair/ python convert_tf_checkpoint_to_pytorch.py \ --tf_checkpoint_path ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_model.ckpt \ --bert_config_file ./source/bert/pretrain_model/cn/chinese_L-12_H-768_A-12/bert_config.json \ --pytorch_dump_path ./source/bert/pretrain_model/cn/pytorch_model.bin 数据准备
cd generate/ python generate_custom_NLI_M.py 然后进行模型训练，演示目的，只训练一个epoch
cd ../ CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_TABSA-v1.py \ --task_name custom_NLI_M \ --data_dir data/custom/bert-pair/ \ --vocab_file .</description></item></channel></rss>